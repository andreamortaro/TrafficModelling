{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d3dc628",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Libraries\n",
    "from scipy.integrate import odeint, solve_ivp\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d554e98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-16 12:49:50.541075: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow/Keras: 2.11.0\n",
      "sklearn: 1.3.0\n"
     ]
    }
   ],
   "source": [
    "## libraries for NN driven model\n",
    "# Tensorflow / Keras\n",
    "from tensorflow import keras # for building Neural Networks\n",
    "print('Tensorflow/Keras: %s' % keras.__version__) # print version\n",
    "from keras.models import Sequential # for creating a linear stack of layers for our Neural Network\n",
    "from keras import Input # for instantiating a keras tensor\n",
    "from keras.layers import Dense # for creating regular densely-connected NN layers.\n",
    "import keras.metrics as metrics\n",
    "\n",
    "# # Data manipulation\n",
    "# import pandas as pd # for data manipulation\n",
    "# print('pandas: %s' % pd.__version__) # print version\n",
    "# import numpy as np # for data manipulation\n",
    "# print('numpy: %s' % np.__version__) # print version\n",
    "\n",
    "# Sklearn\n",
    "import sklearn # for model evaluation\n",
    "print('sklearn: %s' % sklearn.__version__) # print version\n",
    "# from sklearn.model_selection import train_test_split # for splitting data into train and test samples\n",
    "# from sklearn.metrics import classification_report # for model evaluation metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27b63b7",
   "metadata": {},
   "source": [
    "# Define the LWR models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ddce756",
   "metadata": {},
   "source": [
    "## Lin and Log models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d6718d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Traffic dynamic with LWR-model\n",
    "\n",
    "def TD_LWR_model(t, X, N, v0, L, flag):\n",
    "    \"\"\"\n",
    "    Lighthill-Whitham-Richards (LWR) traffic flow model in 1D\n",
    "    \"\"\"\n",
    "    #N, v0, L, flag = args[0], args[1], args[2], args[3]\n",
    "    \n",
    "    # W function\n",
    "    match flag:\n",
    "        case \"Lin\":\n",
    "            W = lambda z: v0*(1-1/z)\n",
    "        case \"Log\":\n",
    "            W = lambda z: v0*np.log(z)\n",
    "        case _:\n",
    "            return f\"No match for {flag}, you can only choose between \\\"Lin\\\" and \\\"Log\\\"\"\n",
    "\n",
    "    # ode sys\n",
    "    d_x = np.zeros(N)\n",
    "    \n",
    "    for i in range(0,N-1):\n",
    "        tmp = (X[i+1] - X[i])/L\n",
    "        d_x[i] = W(tmp)\n",
    "\n",
    "    d_x[N-1] = v0\n",
    "        \n",
    "    return d_x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d6b935",
   "metadata": {},
   "source": [
    "## NN driven model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baaf2365",
   "metadata": {},
   "source": [
    "### Define the Traffic Dynamic model drven by a NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b172bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def havesametype(A,B):\n",
    "    \n",
    "    \"\"\"\n",
    "    check if the two entries A and B are both scalars or both lists\n",
    "    \"\"\"\n",
    "    \n",
    "    result = False\n",
    "    test = [np.isscalar(A), np.isscalar(B)]\n",
    "    \n",
    "    if len(set(test)) == 1:\n",
    "        result = True\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1bd98145",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Traffic dynamic with ANN\n",
    "\n",
    "def TD_ANN_model(t, X, vel):\n",
    "    \n",
    "    \"\"\"\n",
    "    Lighthill-Whitham-Richards (LWR) traffic flow model in 1D\n",
    "    Transform a list as vel into a function of t,X.\n",
    "    \"\"\"\n",
    "    \n",
    "#    # X will be only stored in a list, even if is only a scalar   \n",
    "#     if np.isscalar(vel):\n",
    "#         pass\n",
    "#     else: # both lists\n",
    "#         if len(X) != len(vel):\n",
    "#             return f\"There is a mismatch btw X and vel lenghts as arrays\"\n",
    "\n",
    "    d_x = vel\n",
    "        \n",
    "    return d_x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18217f73",
   "metadata": {},
   "source": [
    "### Ode solver for the NN driven model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c299620",
   "metadata": {},
   "outputs": [],
   "source": [
    "def odesolver_ann(x0, vel, t0, tend, deltat = 0.05):\n",
    "    \n",
    "    \"\"\"\n",
    "    odesolver_ann solves the TD_ANN_model ode system:\n",
    "    * in [t0, tend] with timestep as deltat,\n",
    "    * starting from x0\n",
    "    * vel is the rhs passed to TD_ANN_model to create the model\n",
    "    \"\"\"\n",
    "    \n",
    "    Nt = int((tend-t0)/deltat) + 1          # number of discretization points (we need to cast the value)\n",
    "    tspan_ann = np.linspace(t0, tend, Nt)   # timespan\n",
    "    \n",
    "#     if havesametype(x0,vel) == True:\n",
    "#         sol_ann = odeint(TD_ANN_model, x0, tspan_ann, args=(vel,), tfirst = True).T\n",
    "#     else:\n",
    "#         return f\"X and vel have different dimensions: one is scalar, the other is a list\"\n",
    "\n",
    "    sol_ann = odeint(TD_ANN_model, x0, tspan_ann, args=(vel,), tfirst = True).T\n",
    "\n",
    "    return tspan_ann, sol_ann"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788b99af",
   "metadata": {},
   "source": [
    "### Create and train a neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e09001df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(network):\n",
    "    \n",
    "    \"\"\"\n",
    "    Create the neural network model to define our TD_ANN_model ode system.\n",
    "    \"\"\"\n",
    "    \n",
    "    model = Sequential()\n",
    "\n",
    "    # Input Layer\n",
    "    model.add(Input(shape=(network[0],), name='Input-Layer'))\n",
    "\n",
    "    # Hidden Layers\n",
    "    num_layers = len(network)\n",
    "    for i in range(1,num_layers-1):\n",
    "        label = \"Hidden-Layer-\" + str(i)\n",
    "        #initializer = keras.initializers.RandomUniform(minval=-1., maxval=1.) # init weights in U[-1,1]\n",
    "        ## softplus(x) = log(exp(x) + 1)\n",
    "        model.add(Dense(network[i], activation='softplus',\n",
    "                        use_bias=True,\n",
    "                        bias_initializer = keras.initializers.RandomUniform(minval=-1., maxval=1.), # init weights in U[-1,1]\n",
    "                        kernel_initializer=keras.initializers.RandomUniform(minval=-1., maxval=1.),\n",
    "                        name=label))\n",
    "\n",
    "    # Output Layer\n",
    "    initializer = keras.initializers.RandomUniform(minval=-1., maxval=1.) # init weights in U[-1,1]\n",
    "    ## linear(x) = x\n",
    "    model.add(Dense(network[-1], activation='linear',\n",
    "                    use_bias=True,\n",
    "                    bias_initializer = keras.initializers.RandomUniform(minval=-1., maxval=1.), # init weights in U[-1,1]\n",
    "                    kernel_initializer=keras.initializers.RandomUniform(minval=-1., maxval=1.),\n",
    "                    name='Output-Layer'))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac0f364f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_nn(model, X, y, batch_size=10, epochs=3, verbose='auto'):\n",
    "    \n",
    "    \"\"\"\n",
    "    Compile the neural network model and fit using X and y.\n",
    "    \"\"\"\n",
    "    \n",
    "    ## Compile keras model\n",
    "    model.compile(optimizer='adam',\n",
    "                     loss='mean_squared_error', # Loss function to be optimized.\n",
    "                     metrics=[metrics.mae]      # List of metrics to be evaluated by the model during training and testing.\n",
    "                    )\n",
    "\n",
    "    ## Fit keras model on the dataset\n",
    "    model.fit(X, # input data\n",
    "              y, # target data\n",
    "              batch_size,  # default=32, Number of samples per gradient update.\n",
    "              epochs,      # default=1, Number of epochs to train the model.\n",
    "                           # An epoch is an iteration over the entire x and y data provided\n",
    "              verbose\n",
    "             )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d870000a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stats(model,flag_summary=True):\n",
    "    \"\"\"\n",
    "    function to plot stats of our model\n",
    "    \"\"\"\n",
    "    if flag_summary==True:\n",
    "        print(\"\")\n",
    "        print('-------------------- Model Summary --------------------')\n",
    "        model.summary() # print model summary\n",
    "        print(\"\")\n",
    "    print('-------------------- Weights and Biases --------------------')\n",
    "    for layer in model.layers:\n",
    "        print(\"Layer: \", layer.name) # print layer name\n",
    "        print(\"  --Kernels (Weights): \", layer.get_weights()[0]) # kernels (weights)\n",
    "        print(\"  --Biases: \", layer.get_weights()[1]) # biases\n",
    "\n",
    "    print(\"\")\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04749ec5",
   "metadata": {},
   "source": [
    "### Solve the NN-driven model in a scene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3dcde325",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_ann_scene(scn):\n",
    "    \n",
    "    \"\"\"\n",
    "    create_data_ann_scene gives the X and y for an entire scene\n",
    "    \n",
    "    X_arr, y_arr = create_data_ann_scene(scn)\n",
    "    \"\"\"\n",
    "    \n",
    "    ## Create X\n",
    "    X_arr = scn['Cons Dis']\n",
    "    \n",
    "    ## Create y\n",
    "    dX_arr = np.diff(scn['Xarr'],axis=1)\n",
    "    dT_arr = np.diff(scn['Tarr'])\n",
    "    velocity = dX_arr/dT_arr # velocity at the timestamps\n",
    "\n",
    "    # we choose the first velocity discretized as (x_(i+1)-x_i)/deltaT\n",
    "    y_arr = velocity[:-1] #drop the last vehicle\n",
    "    \n",
    "    return X_arr, y_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f55333fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def odesolver_ann_scene(nn_model, scn, batch_size, epochs, v0, tout=\"end\", verbose=\"auto\"):\n",
    "    \n",
    "    \"\"\"\n",
    "    * tout = \"end\" for the solution at final t\n",
    "    * tout = \"all\" for the solution over all the time interval\n",
    "    \"\"\"\n",
    "    \n",
    "    N = scn['N. vehicles']\n",
    "    tstamps = scn['Tarr']\n",
    "    \n",
    "    vel_ann_list = []\n",
    "    t_ann_list = [scn['Tarr'][0]]\n",
    "    x_ann_list = [[i] for i in scn['Xarr'][:,0]]\n",
    "\n",
    "    X_arr, y_arr = create_data_ann_scene(scn)\n",
    "    \n",
    "    for_end = len(tstamps)-1\n",
    "    \n",
    "    print(f\"We have {for_end} time intervals inside [{tstamps[0]},{tstamps[-1]}]\\n\")\n",
    "    \n",
    "    for i in range(0,for_end):\n",
    "\n",
    "        print(\"--\"*50)\n",
    "        print(f\"Time interval n.{i}: [{scn['Tarr'][i]},{scn['Tarr'][i+1]}]\\n\")\n",
    "        \n",
    "        ## STEP 1: Neural Network\n",
    "        # Create the dataset and train the nn model\n",
    "        X, y = X_arr[:,i], y_arr[:,i]\n",
    "        train_nn(nn_model, X, y, batch_size, epochs, verbose)\n",
    "        \n",
    "        # Predict the rhs of the TD_ANN_model\n",
    "        y_pred = nn_model(X).numpy().flatten().tolist() # To avoid that the funcion is retraced at every call.\n",
    "        vel_ann_list.append(np.append(y_pred,v0).tolist())\n",
    "\n",
    "        print(f\"\\\n",
    "        * y_true: {y}\\n\\\n",
    "        * y_pred: {y_pred}\\n\")\n",
    "        \n",
    "        ## STEP 2: Solve the ODE sys in this time interval\n",
    "        # x0 = scn['Xarr'][:,i]                                     # not use the true value\n",
    "        x0 = [list[-1] for list in np.vstack(x_ann_list).tolist()]  # last values computed\n",
    "        t0, tend = scn['Tarr'][i], scn['Tarr'][i+1]\n",
    "        vel_ann = vel_ann_list[i]\n",
    "        tspan_ann, sol_ann = odesolver_ann(x0, vel_ann, t0, tend, deltat = 0.05)\n",
    "\n",
    "        \n",
    "        ## Only last value\n",
    "        if tout == \"end\":\n",
    "            x_ann = sol_ann[:,-1].tolist()\n",
    "            t_ann = tspan_ann[-1]\n",
    "\n",
    "            # add the solution to the correct vehicle path\n",
    "            for j in range(0,N):\n",
    "                x_ann_list[j].append(x_ann[j])\n",
    "            t_ann_list.append(t_ann)\n",
    "            \n",
    "            \n",
    "        ## In all the time interval\n",
    "        if tout == \"all\":\n",
    "            x_ann = sol_ann.tolist()\n",
    "            t_ann = tspan_ann[1:]\n",
    "\n",
    "            # add the solution to the correct vehicle path\n",
    "            for j in range(0,N):\n",
    "                # drop the first recording, because added before the for\n",
    "                tmp = x_ann[j][1:]\n",
    "                x_ann_list[j] = np.concatenate([x_ann_list[j],tmp])\n",
    "            t_ann_list = np.concatenate([t_ann_list,t_ann]).tolist()\n",
    "\n",
    "        print(\"--\"*50)\n",
    "            \n",
    "    return t_ann_list, x_ann_list, vel_ann_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8fbbe63",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
