{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d554e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ODE\n",
    "from scipy.integrate import odeint, solve_ivp\n",
    "import numpy as np\n",
    "\n",
    "from ipynb.fs.full.myfun_nn import *\n",
    "from ipynb.fs.full.myfun_opt import *\n",
    "from ipynb.fs.defs.myfun_plot import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27b63b7",
   "metadata": {},
   "source": [
    "# Define the LWR models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ddce756",
   "metadata": {},
   "source": [
    "## Lin and Log models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6718d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Traffic dynamic with LWR-model\n",
    "\n",
    "def TD_LWR_model(t, X, N, v0, L, flag):\n",
    "    \"\"\"\n",
    "    Lighthill-Whitham-Richards (LWR) traffic flow model in 1D\n",
    "    \"\"\"\n",
    "    #N, v0, L, flag = args[0], args[1], args[2], args[3]\n",
    "    \n",
    "    # W function\n",
    "    match flag:\n",
    "        case \"Lin\":\n",
    "            W = lambda z: v0*(1-1/z)\n",
    "        case \"Log\":\n",
    "            W = lambda z: v0*np.log(z)\n",
    "        case _:\n",
    "            return f\"No match for {flag}, you can only choose between \\\"Lin\\\" and \\\"Log\\\"\"\n",
    "\n",
    "    # ode sys\n",
    "    d_x = np.zeros(N)\n",
    "    \n",
    "    for i in range(0,N-1):\n",
    "        tmp = (X[i+1] - X[i])/L\n",
    "        d_x[i] = W(tmp)\n",
    "\n",
    "    d_x[N-1] = v0\n",
    "        \n",
    "    return d_x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d6b935",
   "metadata": {},
   "source": [
    "## NN driven model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd98145",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Traffic dynamic with ANN\n",
    "\n",
    "def TD_ANN_model(t, X, vel):\n",
    "    \n",
    "    \"\"\"\n",
    "    Lighthill-Whitham-Richards (LWR) traffic flow model in 1D\n",
    "    Transform a list as vel into a function of t,X.\n",
    "    \"\"\"\n",
    "    \n",
    "    d_x = vel\n",
    "        \n",
    "    return d_x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44fc198",
   "metadata": {},
   "source": [
    "### Ode solver for the NN driven model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62becfdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_discretization(t0, tend, deltat=0.05):\n",
    "    \n",
    "    Nt = round((tend-t0)/deltat) + 1               # number of discretization points\n",
    "                                                   # cast the value into int, us round to avoid cast problem\n",
    "    tspan = np.linspace(t0, tend, int(Nt))         # timespan\n",
    "    \n",
    "    return tspan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c299620",
   "metadata": {},
   "outputs": [],
   "source": [
    "def odesolver_ann(x0, vel, t0, tend, deltat = 0.05):\n",
    "    \n",
    "    \"\"\"\n",
    "    odesolver_ann solves the TD_ANN_model ode system:\n",
    "    * in [t0, tend] with timestep as deltat,\n",
    "    * starting from x0\n",
    "    * vel is the rhs passed to TD_ANN_model to create the model\n",
    "    \"\"\"\n",
    "    \n",
    "    tspan_ann = time_discretization(t0,tend,deltat)\n",
    "        \n",
    "    sol_ann = odeint(TD_ANN_model, x0, tspan_ann, args=(vel,), tfirst = True).T\n",
    "\n",
    "    return tspan_ann, sol_ann"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00022eda",
   "metadata": {},
   "source": [
    "### Create data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dcde325",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_ann_scene(scn):\n",
    "    \n",
    "    \"\"\"\n",
    "    create_data_ann_scene gives the X and y for an entire scene\n",
    "    \n",
    "    X_scn, y_scn = create_data_ann_scene(scn)\n",
    "    \n",
    "    X_scn is a list of consecutive distances btw the vehicles of a scene, at each timestamps\n",
    "    y_scn is a list of approximated velocities, of all the vehicles except the leader one.\n",
    "    \"\"\"\n",
    "    \n",
    "    ## Create X\n",
    "    X_scn = scn['Cons Dis']\n",
    "    \n",
    "    ## Create y\n",
    "    dX_scn = np.diff(scn['Xarr'],axis=1)\n",
    "    dT_scn = np.diff(scn['Tarr'])\n",
    "    velocity = dX_scn/dT_scn # velocity at the timestamps\n",
    "\n",
    "    # we choose the first velocity discretized as (x_(i+1)-x_i)/deltaT\n",
    "    y_scn = velocity[:-1] #drop the last vehicle\n",
    "    \n",
    "    return X_scn, y_scn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99476a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_ann_seq(df):\n",
    "    \n",
    "    \"\"\"\n",
    "    get two arrays of arrays for X and y\n",
    "    \"\"\"\n",
    "    \n",
    "    X_seq, y_seq = [],[]\n",
    "    \n",
    "    # extract input and target for each scene\n",
    "    for row in df.iterrows(): #run over rows\n",
    "        scn = row[1]\n",
    "        X_scn, y_scn = create_data_ann_scene(scn)\n",
    "        \n",
    "        # store in a list\n",
    "        X_seq.append(X_scn)\n",
    "        y_seq.append(y_scn)\n",
    "\n",
    "    return X_seq, y_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1323fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq2scn(df):\n",
    "    \n",
    "    \"\"\"\n",
    "    get an array of scenes, pandas obj\n",
    "    \"\"\"\n",
    "    \n",
    "    seq = []\n",
    "    \n",
    "    # extract input and target for each scene\n",
    "    for row in df.iterrows(): #run over rows\n",
    "        scn = row[1]\n",
    "        seq.append(scn)\n",
    "\n",
    "    return seq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04749ec5",
   "metadata": {},
   "source": [
    "### Solve the NN-driven model in a scene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55333fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def odesolver_ann_scene(nn_model, scn, epochs, batch_size, v0, deltat=0.05, verbose=\"auto\"):\n",
    "    \n",
    "    \"\"\"\n",
    "    t_ann_list, x_ann_list, vel_ann_list = odesolver_ann_scene(nn_model, scn, epochs, batch_size, v0, deltat=0.05, verbose=\"auto\")\n",
    "    \n",
    "    odesolver_ann_scene solve the ode model driven by a nn in a scene.\n",
    "    \"\"\"\n",
    "    \n",
    "    N = scn['N. vehicles']\n",
    "    tstamps = scn['Tarr']\n",
    "    \n",
    "    vel_ann_list = []\n",
    "    t_ann_list = [scn['Tarr'][0]]\n",
    "    x_ann_list = [[i] for i in scn['Xarr'][:,0]]\n",
    "\n",
    "    X_arr, y_arr = create_data_ann_scene(scn)\n",
    "    \n",
    "    formatter = '{0:.02f}'\n",
    "    \n",
    "    print(\"==\"*50)\n",
    "    print(f\"We have {len(tstamps)-1} time intervals inside [{formatter.format(tstamps[0])},{formatter.format(tstamps[-1])}]\\n\")\n",
    "    \n",
    "    for i in range(0,len(tstamps)-1):\n",
    "\n",
    "        print(\"--\"*50)\n",
    "        print(f\"Time interval n.{i}: [{formatter.format(scn['Tarr'][i])}, {formatter.format(scn['Tarr'][i+1])}]\\n\")\n",
    "        \n",
    "        ## STEP 1: Neural Network\n",
    "        # Create the dataset and train the nn model\n",
    "        X, y = X_arr[:,i], y_arr[:,i]\n",
    "        train_nn(nn_model, X, y, epochs, batch_size, verbose)\n",
    "        \n",
    "        # Predict the rhs of the TD_ANN_model\n",
    "        y_pred = nn_model(X).numpy().flatten().tolist()\n",
    "        vel_ann_list.append(np.append(y_pred,v0).tolist())\n",
    "\n",
    "        print(f\"\\\n",
    "        * y_true: {y}\\n\\\n",
    "        * y_pred: {y_pred}\\n\")\n",
    "        \n",
    "        ## STEP 2: Solve the ODE sys in this time interval\n",
    "        x0 = [l[-1] for l in np.vstack(x_ann_list).tolist()]  # last values computed\n",
    "        t0, tend = scn['Tarr'][i], scn['Tarr'][i+1]\n",
    "        vel_ann = vel_ann_list[i]\n",
    "        tspan_ann, sol_ann = odesolver_ann(x0, vel_ann, t0, tend, deltat)\n",
    "\n",
    "        ## STEP 3: Update x_ann_list, t_ann_list\n",
    "        x_ann = sol_ann.tolist()\n",
    "        t_ann = tspan_ann[1:] # avoid the first recording\n",
    "\n",
    "        # add sol to the correct veh\n",
    "        for j in range(0,N):\n",
    "            tmp = x_ann[j][1:] # avoid the first recording\n",
    "            x_ann_list[j] = np.concatenate([x_ann_list[j],tmp])\n",
    "        t_ann_list = np.concatenate([t_ann_list,t_ann]).tolist()\n",
    "\n",
    "        print(\"--\"*50)\n",
    "    \n",
    "    print(\"==\"*50)\n",
    "    \n",
    "    return t_ann_list, x_ann_list, vel_ann_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80cbf531",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_timestamps_scene(t, x, deltat = 0.05):\n",
    "    \n",
    "    \"\"\"\n",
    "    Match the computed solution to the same timestamps of the scene\n",
    "    \n",
    "    t_matched, x_matched = match_timestamps_scene(t, x, deltat = 0.05)\n",
    "    \"\"\"\n",
    "    # To recover the same timestep in the data\n",
    "    factor = int(0.2/deltat)\n",
    "    \n",
    "    t_matched = np.array(t)[::factor]\n",
    "    x_matched = np.array([traj[::factor] for traj in x])\n",
    "    \n",
    "    return t_matched, x_matched"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36030428",
   "metadata": {},
   "source": [
    "### Solve the NN driven model in a scene, with custom training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3447d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_nn_scn(model, scn, v0, LEARNING_RATE):\n",
    "    \n",
    "    \"\"\"\n",
    "    t_list, x_list, v_list = solve_scn(model, scn, v0, LEARNING_RATE)\n",
    "    \"\"\"\n",
    "    \n",
    "    x_list, t_list, v_list = [[i] for i in scn['Xarr'][:,0]], [scn['Tarr'][0]], []\n",
    "    N, tstamps = scn['N. vehicles'], scn['Tarr']\n",
    "    \n",
    "    X_arr, y_arr = create_data_ann_scene(scn)\n",
    "    \n",
    "    # for nn training\n",
    "    optimizer = tf.keras.optimizers.SGD(learning_rate=LEARNING_RATE)\n",
    "    loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "    \n",
    "    for i in range(0,len(tstamps)-1):\n",
    "\n",
    "        ## STEP 1: Create the dataset and train the nn model\n",
    "        X, y = X_arr[:,i], y_arr[:,i]\n",
    "\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "\n",
    "            # Create tensor that you will watch\n",
    "            x_tensor = tf.convert_to_tensor(X, dtype=tf.float64)\n",
    "            tape.watch(x_tensor)\n",
    "\n",
    "            y_pred = model(X, training=True) # forward pass\n",
    "            loss_value = loss_fn(y_true = y, y_pred = y_pred) # loss function\n",
    "\n",
    "        # Compute gradients\n",
    "        trainable_vars = model.trainable_variables\n",
    "        grads = tape.gradient(loss_value, trainable_vars)\n",
    "\n",
    "        # Update weights\n",
    "        optimizer.apply_gradients(zip(grads, trainable_vars))\n",
    "\n",
    "#         print_grads(model, grads) # plot the last grads wrt trainable vars\n",
    "\n",
    "        # Store the prediction\n",
    "        v_list.append(np.append(y_pred.numpy().flatten().tolist(),v0).tolist())\n",
    "\n",
    "        ## STEP 2: Solve the ODE sys in this time interval\n",
    "        x0 = [l[-1] for l in np.vstack(x_list).tolist()]  # last values computed\n",
    "        t0, tend = scn['Tarr'][i], scn['Tarr'][i+1]\n",
    "        v_ann = v_list[i]\n",
    "        tspan_ann, sol_ann = odesolver_ann(x0, v_ann, t0, tend, deltat=0.05)\n",
    "\n",
    "        ## STEP 3: Update x_ann_list, t_ann_list\n",
    "        x_ann = sol_ann.tolist()\n",
    "        t_ann = tspan_ann[1:] # avoid the first recording\n",
    "\n",
    "        # add sol to the correct veh\n",
    "        for j in range(0,N):\n",
    "            tmp = x_ann[j][1:] # avoid the first recording\n",
    "            x_list[j] = np.concatenate([x_list[j],tmp])\n",
    "        t_list = np.concatenate([t_list,t_ann]).tolist()\n",
    "#             print(\"--\"*50)\n",
    "\n",
    "    return t_list, x_list, v_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3e52f5",
   "metadata": {},
   "source": [
    "### Solve the nn-driven model in the all the scenes in a df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d32dbce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_nn_df(df, model, v0_guess=30, NUM_EPOCHS=10, LEARNING_RATE_NN=0.01, LEARNING_RATE_v0=0.5, flag_print=False):\n",
    "    \n",
    "    info_df = []\n",
    "    scn_list = seq2scn(df)\n",
    "    fmt = '{0:.02f}'\n",
    "    \n",
    "    for scn in scn_list:\n",
    "    #for scn in scn_list:\n",
    "\n",
    "        v0, v0_scn = v0_guess, []\n",
    "        tstamps = scn['Tarr']\n",
    "\n",
    "        print(f\"\\nScene n.{scn.name}/{len(scn_list)}, time interval:\\\n",
    "[{fmt.format(tstamps[0])},{fmt.format(tstamps[-1])}]\")\n",
    "\n",
    "        for epoch in range(NUM_EPOCHS+1):\n",
    "\n",
    "            t_list, x_list, _ = solve_nn_scn(model, scn, v0, LEARNING_RATE_NN)\n",
    "            _, x_list_matched = match_timestamps_scene(t_list, x_list)\n",
    "\n",
    "            # Append v0 used, before to update it\n",
    "            v0_scn.append(v0)\n",
    "\n",
    "            # Update v0 with SGD\n",
    "            v0_upd, loss_val, grads, g = SGD_v0(scn, x_list_matched, v0, LEARNING_RATE_v0)\n",
    "            v0 = v0_upd\n",
    "\n",
    "#             if epoch % nprint == 0:\n",
    "#                 print(\"--\"*50)\n",
    "#                 print(f\"Epoch n.{epoch}\")\n",
    "\n",
    "#                 # plot function\n",
    "#                 t_ann_matched, trajs_ann_matched = match_timestamps_scene(t_list, x_list)\n",
    "#                 tscale = 1+(tstamps[-1]-tstamps[0])/20000\n",
    "#                 title = f\"Scene n. {scn.name}, at epoch n.{epoch}\"\n",
    "#                 plot_scn(scn, trajs_ann_matched, title, xbal=0.01, ybal=0.05, scale=tscale)\n",
    "\n",
    "#                 print(f\"\\n\\\n",
    "#                 Loss: {loss_val}\\n\\\n",
    "#                 gradient to update v0: {g}\\n\\\n",
    "#                 v0 updated: {v0}\")\n",
    "#                 print(\"--\"*50)\n",
    "\n",
    "        v0_scn_mean = np.array(v0_scn).mean()\n",
    "        info_df.append([scn.name,v0_scn_mean])\n",
    "\n",
    "        # plot function\n",
    "        if flag_print:\n",
    "            print(\"--\"*50)\n",
    "            t_ann_matched, trajs_ann_matched = match_timestamps_scene(t_list, x_list)\n",
    "            tscale = 1+(tstamps[-1]-tstamps[0])/20000\n",
    "            title = f\"$Scene\\ n.{scn.name}/{len(scn_list)},\\ using\\ {epoch}\\ epochs$\"\n",
    "            plot_scn(scn, trajs_ann_matched, title, xbal=0.01, ybal=0.05, scale=tscale)\n",
    "            print(f\"\\\n",
    "            Loss: {loss_val}\\n\\\n",
    "            v0 updated: {v0}\")\n",
    "            print(\"--\"*50)\n",
    "\n",
    "    return info_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea9456b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
