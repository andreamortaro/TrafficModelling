{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d554e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import scipy.optimize as optimize\n",
    "from scipy.integrate import odeint, solve_ivp\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Import custom functions from other notebooks\n",
    "from ipynb.fs.full.myfun_nn import *\n",
    "from ipynb.fs.full.myfun_model_usefulfuns import *\n",
    "from ipynb.fs.defs.myfun_plot import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d6b935",
   "metadata": {},
   "source": [
    "# NN driven model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3633f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TD_ANN_model(t, X, vel):\n",
    "    \"\"\"\n",
    "    Lighthill-Whitham-Richards (LWR) traffic flow model in 1D.\n",
    "\n",
    "    Args:\n",
    "    - t: Time parameter (not used in this function).\n",
    "    - X: Spatial parameter representing location.\n",
    "    - vel: Velocity data that influences the traffic dynamics.\n",
    "\n",
    "    Returns:\n",
    "    - d_x: The rate of change of traffic density (not explicitly calculated here, it's based on velocity).\n",
    "    \"\"\"\n",
    "    \n",
    "    # In this simple model, the rate of change of traffic density is determined by the velocity.\n",
    "    d_x = vel\n",
    "        \n",
    "    return d_x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44fc198",
   "metadata": {},
   "source": [
    "## Ode solver for the NN driven model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88c588d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def odesolver_ann(x0, vel, t0, tend, deltat=0.05):\n",
    "    \"\"\"\n",
    "    Solve the TD_ANN_model ODE system using the odeint solver.\n",
    "\n",
    "    Args:\n",
    "    - x0: Initial condition.\n",
    "    - vel: Velocity data that influences the traffic dynamics.\n",
    "    - t0: Initial time.\n",
    "    - tend: End time.\n",
    "    - deltat: Time step (default: 0.05).\n",
    "\n",
    "    Returns:\n",
    "    - tspan_ann: Time points at which the solution is computed.\n",
    "    - sol_ann: Solution to the ODE system at the specified time points.\n",
    "    \"\"\"\n",
    "\n",
    "    tspan_ann = time_discretization(t0, tend, deltat)  # Generate a time array with discrete time points.\n",
    "\n",
    "    # Use odeint to solve the ODE system defined by TD_ANN_model.\n",
    "    sol_ann = odeint(TD_ANN_model, x0, tspan_ann, args=(vel,), tfirst=True).T\n",
    "\n",
    "    return tspan_ann, sol_ann"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dcde325",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_ann_scene(scn):\n",
    "    \"\"\"\n",
    "    Create data for training an ANN scene.\n",
    "\n",
    "    Args:\n",
    "    - scn: A dictionary or data structure representing a scene.\n",
    "\n",
    "    Returns:\n",
    "    - X_scn: A list of consecutive distances between the vehicles in the scene at each timestamp.\n",
    "    - y_scn: A list of approximated velocities for all vehicles except the leader one.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create X: List of consecutive distances between vehicles in the scene.\n",
    "    X_scn = scn['cons_dis']\n",
    "\n",
    "    # Create y: List of approximated velocities.\n",
    "    dX_scn = np.diff(scn['Xarr'], axis=1)\n",
    "    dT_scn = np.diff(scn['Tarr'])\n",
    "    velocity = dX_scn / dT_scn  # Calculate velocity at timestamps\n",
    "\n",
    "    # We choose the first velocity discretized as (x_(i+1) - x_i) / deltaT\n",
    "    y_scn = velocity[:-1]  # Exclude the last vehicle (leader)\n",
    "\n",
    "    return X_scn, y_scn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36030428",
   "metadata": {},
   "source": [
    "### Custom training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a89ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_step(model, scn, v0, it, lists, nn_fun):\n",
    "    \"\"\"\n",
    "    Perform a single iteration of solving a scene's dynamics step by step.\n",
    "\n",
    "    Args:\n",
    "    - model: The neural network model to be used for prediction.\n",
    "    - scn: A dictionary or data structure representing a scene.\n",
    "    - v0: Initial velocity (leader vehicle's velocity).\n",
    "    - it: Current iteration.\n",
    "    - PLOT_ITER: Number of iterations to plot.\n",
    "    - lists: Lists of time points, distances between vehicles, and predicted velocities.\n",
    "    - nn_fun: Tuple containing loss function and optimizer.\n",
    "\n",
    "    Returns:\n",
    "    - t_list: List of time points.\n",
    "    - x_list: List of distances between vehicles over time.\n",
    "    - v_list: List of predicted velocities for each vehicle.\n",
    "    \"\"\"\n",
    "\n",
    "    t_list, x_list, v_list = lists\n",
    "    X_arr, y_arr = create_data_ann_scene(scn)\n",
    "    N, tstamps, fmt = scn['N. vehicles'], scn['Tarr'], '{0:.02f}'\n",
    "\n",
    "    loss_fn, optimizer = nn_fun\n",
    "    \n",
    "    for i in range(0, len(tstamps) - 1):\n",
    "                \n",
    "        # STEP 1: Create the dataset and train the NN model\n",
    "        X, y = X_arr[:, i], y_arr[:, i]\n",
    "        \n",
    "        # Train the NN and update model coefficients\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "\n",
    "            # Create a tensor that you will watch\n",
    "            x_tensor = tf.convert_to_tensor(X, dtype=tf.float64)\n",
    "            tape.watch(x_tensor)\n",
    "\n",
    "            y_pred = model(X, training=True)  # Forward pass\n",
    "            loss_value = loss_fn(y_true=y, y_pred=y_pred)  # Loss function          \n",
    "\n",
    "        # Compute gradients\n",
    "        trainable_vars = model.trainable_variables\n",
    "        grads = tape.gradient(loss_value, trainable_vars)\n",
    "        \n",
    "        # Update weights\n",
    "        optimizer.apply_gradients(zip(grads, trainable_vars))\n",
    "        \n",
    "        # STEP 2: Solve the ODE system in this time interval\n",
    "        x0 = [l[-1] for l in np.vstack(x_list).tolist()]  # Last values computed\n",
    "        t0, tend = scn['Tarr'][i], scn['Tarr'][i + 1]\n",
    "        v_ann = np.append(y_pred.numpy().flatten().tolist(), v0).tolist()\n",
    "        tspan_ann, sol_ann = odesolver_ann(x0, v_ann, t0, tend, deltat=0.05)    \n",
    "        \n",
    "        # STEP 3: Store the information\n",
    "        x_list, t_list = update_sol_lists(N, tspan_ann, sol_ann, x_list, t_list)\n",
    "        v_list.append(v_ann)\n",
    "        \n",
    "    return t_list, x_list, v_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1fa092",
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_nn_scn(model, scn, v0, LEARNING_RATE_NN=0.001, LEARNING_RATE_v0=0.5, NUM_ITER=200, info_flag=True):\n",
    "    \"\"\"\n",
    "    Solve a scene's dynamics with custom training of a neural network model and updating v0 using SGD.\n",
    "\n",
    "    Args:\n",
    "    - model: The neural network model to be used for prediction.\n",
    "    - scn: A dictionary or data structure representing a scene.\n",
    "    - v0: Initial velocity (leader vehicle's velocity).\n",
    "    - LEARNING_RATE_NN: Learning rate for the neural network (default: 0.001).\n",
    "    - LEARNING_RATE_v0: Learning rate for updating v0 using SGD (default: 0.5).\n",
    "    - NUM_ITER: Number of iterations (default: 200).\n",
    "    - PLOT_ITER: Interval for printing information (default: 25).\n",
    "\n",
    "    Returns:\n",
    "    - t_best: List of time points for the best iteration.\n",
    "    - x_best: List of distances between vehicles for the best iteration.\n",
    "    - v_best: List of predicted velocities for each vehicle for the best iteration.\n",
    "    - v0_scn: List of updated v0 values for each iteration.\n",
    "    - it: Number of iterations.\n",
    "    \"\"\"\n",
    "    \n",
    "    N, tstamps, fmt = scn['N. vehicles'], scn['Tarr'], '{0:.02f}'\n",
    "    v0_scn = []\n",
    "    \n",
    "    print(\"--\"*50)\n",
    "    print(f\"We have {len(tstamps) - 1} time intervals inside [{fmt.format(tstamps[0])},{fmt.format(tstamps[-1])}]\")\n",
    "\n",
    "    # Setting learning rate for SGD\n",
    "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "                    initial_learning_rate=LEARNING_RATE_NN,\n",
    "                    decay_steps=int(NUM_ITER/2) + 1,\n",
    "                    decay_rate=0.9,\n",
    "                    staircase=True)\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.SGD(learning_rate=lr_schedule)\n",
    "    loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "    \n",
    "    err_list, err, diff = [], 1e9, 1\n",
    "    err_list.append(err)\n",
    "    \n",
    "    it = 1\n",
    "    err_best, it_best = err, it\n",
    "    \n",
    "    while (diff > 1e-6 and it < NUM_ITER + 1):\n",
    "        \n",
    "        ## STEP 1: Simulate the dynamics over a scene with v0\n",
    "        t_list, x_list, v_list = [scn['Tarr'][0]], [[i] for i in scn['Xarr'][:, 0]] , []\n",
    "        t_list, x_list, v_list = solve_step(model, scn, v0, it,\n",
    "                                            lists=[t_list, x_list, v_list],\n",
    "                                            nn_fun=[loss_fn, optimizer])\n",
    "        \n",
    "        _, sol_ann_matched = match_timestamps_scene(t_list, x_list)\n",
    "\n",
    "        \n",
    "        ## STEP 2: Update v0 with SGD\n",
    "        v0_upd, loss_val, grads, g = SGD_v0(scn, sol_ann_matched, v0, LEARNING_RATE_v0) \n",
    "        v0_scn.append(v0)\n",
    "        v0 = v0_upd\n",
    "        \n",
    "        ## STEP 3: update params according to the update rule\n",
    "        err = loss_fn(y_true=scn['Xarr'], y_pred=sol_ann_matched).numpy()\n",
    "        err_list.append(err)\n",
    "\n",
    "        if err < err_best:\n",
    "            t_best, x_best, v_best = t_list, x_list, v_list\n",
    "            it_best, err_best = it, err\n",
    "        \n",
    "        # Update diff\n",
    "        if it % 50 == 0:\n",
    "             diff = abs(err_list[-1] - err_list[-50])\n",
    "   \n",
    "        it += 1\n",
    "\n",
    "    ## STEP 4: Print some info\n",
    "    if info_flag:\n",
    "        _, y_arr = create_data_ann_scene(scn)\n",
    "        for i in range(0, len(tstamps) - 1):\n",
    "            print(f\"\\\n",
    "            Interval n.{i}: [{fmt.format(scn['Tarr'][i])}, {fmt.format(scn['Tarr'][i+1])}]\\n\\\n",
    "                * y_true: {y_arr[:, i]}\\n\\\n",
    "                * v_ann: {v_best[i]}\\n\")\n",
    "        \n",
    "        print(f\"\\\n",
    "        Some info:\\n\\\n",
    "        * MSE = {err}\\n\\\n",
    "        * Learning rate NN = {optimizer.learning_rate.numpy()}\\n\\\n",
    "        * diff (update rule for LR NN) = {diff}\\n\\\n",
    "        * It = {it-1}\")\n",
    "        \n",
    "    # Plot function\n",
    "    tscale = 1 + (tstamps[-1] - tstamps[0]) / 10000\n",
    "    title = f\"$df\\  n.\\ {scn['N. file']}\\ -\\ Scene\\ n.\\ {scn.name+1},\\ at\\ it={it - 1}$\"\n",
    "    plot_scn(scn, sol_ann_matched, title, xbal=0.01, ybal=0.05, scale=tscale)\n",
    "\n",
    "    print(\"--\"*50)\n",
    "    \n",
    "    return t_best, x_best, v_best, v0_scn, it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3e52f5",
   "metadata": {},
   "source": [
    "### Solve the nn-driven model in the all the scenes in a df, and get v0 mean for each scene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9738f03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_finder(model, scn, v0):\n",
    "    \"\"\"\n",
    "    Find the best learning rate for a model using a learning rate range test.\n",
    "\n",
    "    Args:\n",
    "    - model: The neural network model to be used for prediction.\n",
    "    - scn: A dictionary or data structure representing a scene.\n",
    "    - v0: Initial velocity (leader vehicle's velocity).\n",
    "\n",
    "    Returns:\n",
    "    - err_lr_best: Best error obtained during the learning rate range test.\n",
    "    - lr_best: Best learning rate found.\n",
    "    - it_lr_best: Number of iterations for the best learning rate.\n",
    "    \"\"\"\n",
    "\n",
    "    X_arr, y_arr = create_data_ann_scene(scn)\n",
    "    lr_range = [0.01, 0.005, 0.001, 0.0005, 0.0001, 0.00005, 0.00001]\n",
    "    err_lr_best, lr_best = 1e9, None\n",
    "\n",
    "    for lr in lr_range:\n",
    "\n",
    "        mmodel = tf.keras.models.clone_model(model)\n",
    "        optimizer = tf.keras.optimizers.SGD(learning_rate=lr)\n",
    "        loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "        for it in range(25):\n",
    "            \n",
    "            t_list, x_list, v_list = [scn['Tarr'][0]], [[i] for i in scn['Xarr'][:,0]], []\n",
    "            t_list, x_list, v_list = solve_step(mmodel, scn, v0, -1,\n",
    "                                                lists=[t_list, x_list, v_list],\n",
    "                                                nn_fun=[loss_fn, optimizer])\n",
    "            \n",
    "            _, sol_ann_matched = match_timestamps_scene(t_list, x_list)            \n",
    "\n",
    "        err = loss_fn(y_true=scn['Xarr'], y_pred=sol_ann_matched).numpy()  \n",
    "\n",
    "        if err < err_lr_best:\n",
    "            err_lr_best, lr_best, it_lr_best = err, lr, it\n",
    "        \n",
    "    return err_lr_best, lr_best, it_lr_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1aa75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SGD_v0(scn, x_list_matched, v0, LEARNING_RATE_v0):\n",
    "    \"\"\"\n",
    "    Update v0 using stochastic gradient descent (SGD) based on a loss between true and predicted trajectories.\n",
    "\n",
    "    Args:\n",
    "    - scn: A dictionary or data structure representing a scene.\n",
    "    - x_list_matched: List of distances between vehicles for matched timestamps.\n",
    "    - v0: Initial velocity (leader vehicle's velocity).\n",
    "    - LEARNING_RATE_v0: Learning rate for updating v0.\n",
    "\n",
    "    Returns:\n",
    "    - v0_upd: Updated v0 after the SGD step.\n",
    "    - loss_val: Loss value based on the comparison of true and predicted trajectories.\n",
    "    - grads: Gradients computed during the optimization.\n",
    "    - g: Mean gradient of the leader vehicle's velocity.\n",
    "    \"\"\"\n",
    "\n",
    "    loss_objective = tf.keras.losses.MeanSquaredError()\n",
    "    \n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "\n",
    "        trajs_true_tensor = tf.convert_to_tensor(scn['Xarr'], dtype=tf.float64)\n",
    "\n",
    "        # Create tensor that you will watch\n",
    "        trajs_pred_tensor = tf.convert_to_tensor(x_list_matched, dtype=tf.float64)\n",
    "        tape.watch(trajs_pred_tensor)\n",
    "\n",
    "        loss_val = loss_objective(y_true=trajs_true_tensor, y_pred=trajs_pred_tensor)\n",
    "\n",
    "    # Compute gradients\n",
    "    grads = tape.gradient(loss_val, trajs_pred_tensor)\n",
    "\n",
    "    # Update v0 based on the mean gradient of the leader vehicle's velocity\n",
    "    g = grads[-1].numpy()[1:].mean()  # Watching at the mean velocity of the leader car\n",
    "    v0_upd = v0 - LEARNING_RATE_v0 * g\n",
    "    \n",
    "    return v0_upd, loss_val, grads, g\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895f9eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_nn_df(df, model, v0, NUM_ITER, LEARNING_RATE_v0):\n",
    "    \"\"\"\n",
    "    Solve the neural network model in a single DataFrame and get information for each scene.\n",
    "\n",
    "    Args:\n",
    "    - df: The DataFrame containing scene data.\n",
    "    - doe: The neural network model architecture.\n",
    "    - v0: Initial velocity guess.\n",
    "    - NUM_ITER: Number of iterations.\n",
    "    - LEARNING_RATE_v0: Learning rate for updating v0 (default: 0.5).\n",
    "\n",
    "    Returns:\n",
    "    - info_df: A DataFrame containing information about each scene's results.\n",
    "    \"\"\"\n",
    "\n",
    "    scn_list = seq2scn(df)\n",
    "    info_scn, fmt, mse_list = [], '{0:.02f}', []\n",
    "    loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "    \n",
    "    for scnnum, scn in enumerate(scn_list):\n",
    "        \n",
    "        tstamps = scn['Tarr']\n",
    "        err_lr_best, lr_best, it_lr_best = lr_finder(model, scn, v0)\n",
    "\n",
    "        print(f\"DataFrame n.{scn['N. file']}. Scene n.{scnnum+1}/{len(scn_list)}\")\n",
    "    \n",
    "        t_list, x_list, v_list, v0_scn, it = solve_nn_scn(model, scn, v0,\n",
    "                                                            lr_best,\n",
    "                                                            LEARNING_RATE_v0,\n",
    "                                                            NUM_ITER, info_flag=False)\n",
    "\n",
    "        # Compute MSE for the solution computed\n",
    "        _, sol_ann_matched = match_timestamps_scene(t_list, x_list)\n",
    "        mse = loss_fn(y_true=scn['Xarr'], y_pred=sol_ann_matched).numpy()\n",
    "        mse_list.append(mse)\n",
    "        \n",
    "        info_scn.append([t_list, x_list, v_list, v0_scn[-1], scn.name, it-1])\n",
    "\n",
    "        print(f\"\\\n",
    "        For scene {scnnum+1}/{len(scn_list)}:\\n\\\n",
    "        * After LR finder: LR_NN={lr_best} with mse={err_lr_best} at it={it_lr_best}\\n\\\n",
    "        * v0 = {v0_scn[-1]}\\n\\\n",
    "        * MSE = {mse}\\n\\\n",
    "        * iterations = {it-1}\")\n",
    "        print(\"--\"*50)\n",
    "        print(\"\\n\")\n",
    "\n",
    "    # Transpose info_df for better handling\n",
    "    tmp = list(map(list, zip(*info_scn)))\n",
    "\n",
    "    mse_mean = np.mean(mse_list)\n",
    "    \n",
    "    info_df = pd.DataFrame({\n",
    "        't_list': tmp[0],\n",
    "        'x_list': tmp[1],\n",
    "        'v_list': tmp[2],\n",
    "        'v0': tmp[3],\n",
    "        'n_scn': tmp[4],\n",
    "        'iter': tmp[5],\n",
    "        'mse': mse_list\n",
    "    })\n",
    "\n",
    "    return info_df, mse_mean, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff50b6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_nn_df(df, model, v0, plot_flag=True):\n",
    "    \"\"\"\n",
    "    Test a neural network model on a DataFrame of scenes and return evaluation information.\n",
    "\n",
    "    Args:\n",
    "    - df: DataFrame containing scene data.\n",
    "    - model: Trained neural network model.\n",
    "    - v0: Initial velocity guess.\n",
    "\n",
    "    Returns:\n",
    "    - info_df: DataFrame containing information about the evaluation.\n",
    "    - mse_mean: Mean squared error across all scenes.\n",
    "    - model: The trained neural network model.\n",
    "    \"\"\"\n",
    "\n",
    "    scn_list = seq2scn(df)\n",
    "    info_scn, fmt, mse_list = [], '{0:.02f}', []\n",
    "    loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "    # Run over scenes\n",
    "    for scnnum, scn in enumerate(scn_list):\n",
    "\n",
    "        N, tstamps = scn['N. vehicles'], scn['Tarr']\n",
    "        X_arr, y_arr = create_data_ann_scene(scn)\n",
    "\n",
    "        print(f\"DataFrame n.{scn['N. file']}. Scene n.{scnnum + 1}/{len(scn_list)}\")\n",
    "\n",
    "        # Initialize lists\n",
    "        t_list, x_list, v_list = [scn['Tarr'][0]], [[i] for i in scn['Xarr'][:, 0]], []\n",
    "\n",
    "        # Solve the ODE\n",
    "        for i in range(0, len(tstamps) - 1):\n",
    "            \n",
    "            X, y = X_arr[:, i], y_arr[:, i]\n",
    "\n",
    "            # Solve the ODE system in this time interval\n",
    "            x0 = [i for i in scn['Xarr'][:, i]]\n",
    "            t0, tend = scn['Tarr'][i], scn['Tarr'][i + 1]\n",
    "            y_pred = model(X)\n",
    "            v_ann = np.append(y_pred.numpy().flatten().tolist(), v0).tolist()\n",
    "            tspan_ann, sol_ann = odesolver_ann(x0, v_ann, t0, tend, deltat=0.05)\n",
    "\n",
    "            # Store the information\n",
    "            x_list, t_list = update_sol_lists(N, tspan_ann, sol_ann, x_list, t_list)\n",
    "            v_list.append(v_ann)\n",
    "\n",
    "        # Evaluate the error\n",
    "        _, sol_ann_matched = match_timestamps_scene(t_list, x_list)\n",
    "        mse = loss_fn(y_true=scn['Xarr'], y_pred=sol_ann_matched).numpy()\n",
    "        mse_list.append(mse)\n",
    "\n",
    "        info_scn.append([t_list, x_list, v_list, scn.name])\n",
    "        \n",
    "        # Plot function\n",
    "        if plot_flag:\n",
    "            tscale = 1 + (tstamps[-1] - tstamps[0]) / 10000\n",
    "            title = f\"$df\\  n.\\ {scn['N. file']}\\ -\\ Scene\\ n.\\ {scnnum+1} - Test$\"\n",
    "            plot_scn(scn, sol_ann_matched, title, xbal=0.01, ybal=0.05, scale=tscale)\n",
    "\n",
    "        print(f\"\\\n",
    "        For scene {scnnum + 1}/{len(scn_list)}:\\n\\\n",
    "        * MSE = {mse}\")\n",
    "        print(\"--\" * 50)\n",
    "        print(\"\\n\")\n",
    "\n",
    "    # Transpose info_df for better handling\n",
    "    tmp = list(map(list, zip(*info_scn)))\n",
    "\n",
    "    mse_mean = np.mean(mse_list)\n",
    "\n",
    "    info_df = pd.DataFrame({\n",
    "        't_list': tmp[0],\n",
    "        'x_list': tmp[1],\n",
    "        'v_list': tmp[2],\n",
    "        'n_scn': tmp[3],\n",
    "        'v0': v0,\n",
    "        'mse': mse_list\n",
    "    })\n",
    "\n",
    "    return info_df, mse_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ab6086",
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_nn_df(df, model, v0, NUM_ITER, LEARNING_RATE_v0=0.5, test=0.3, plot_flag=True):\n",
    "    \"\"\"\n",
    "    Train and test a neural network model on a DataFrame of scenes.\n",
    "\n",
    "    Args:\n",
    "    - df: DataFrame containing scene data.\n",
    "    - model: Neural network model.\n",
    "    - v0: Initial velocity guess.\n",
    "    - NUM_ITER: Number of iterations for training.\n",
    "    - LEARNING_RATE_v0: Learning rate for v0 optimization.\n",
    "    - test_size: Proportion of the dataset to include in the test split.\n",
    "\n",
    "    Returns:\n",
    "    - info_df: DataFrame containing information about the evaluation.\n",
    "    \"\"\"\n",
    "\n",
    "    df_train, df_test = train_test_split(df, test_size = test, random_state=42)\n",
    "        \n",
    "    print(f\"In DataFrame n.{df['N. file'][0]} we have {len(df)} scenes.\\n\\\n",
    "    To train the model we use {len(df_train)} scenes, the remaining {len(df_test)} to test the model.\")\n",
    "    \n",
    "    ## TRAINING\n",
    "    print(f\"\\nTraining step. ({len(df_train)} scenes)\")\n",
    "    print(\"==\"*50)\n",
    "    info_train_df, mse_train, model_trained = train_nn_df(df_train, model, v0, NUM_ITER, LEARNING_RATE_v0)\n",
    "\n",
    "    print(f\"MSE train: {mse_train}\")\n",
    "\n",
    "    print(\"- -\" * 35)\n",
    "\n",
    "    ## TESTING\n",
    "    print(f\"\\nTesting step. ({len(df_test)} scenes)\")\n",
    "    print(\"==\"*50)\n",
    "    \n",
    "    v0_trained = np.mean(info_train_df['v0'])\n",
    "    info_test_df, mse_test = test_nn_df(df_test, model_trained, v0_trained, plot_flag)\n",
    "    \n",
    "    print(f\"MSE test: {mse_test}\\n\")\n",
    "    \n",
    "    print(\"- -\" * 35)\n",
    "\n",
    "    print(f\"\\n\\\n",
    "    Summing up:\\n\\\n",
    "          * MSE train: {mse_train}\\n\\\n",
    "          * MSE test: {mse_test}\\n\\\n",
    "          \")\n",
    "\n",
    "    print(\"- -\" * 35)\n",
    "    \n",
    "    print(\"\\n\\n\\n\")\n",
    "    \n",
    "    # Concatenating info_df\n",
    "    info_train_df['type']=\"train\"\n",
    "    info_train_df['mse_mean']=mse_train\n",
    "\n",
    "    info_test_df['type']=\"test\"\n",
    "    info_test_df['mse_mean']=mse_test\n",
    "\n",
    "    info_df = pd.concat([info_train_df, info_test_df], sort=False)\n",
    "    info_df['N. file'] = df['N. file'][0]\n",
    "    \n",
    "    return info_df, model_trained"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1085ae",
   "metadata": {},
   "source": [
    "### Solve NN driven model in each df of a dataset, and get v0 mean for each scn in each df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2b6c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_nn_dataset(dataset, doe, v0, processed_flag, NUM_ITER, LEARNING_RATE_v0=0.5, test=0.3, plot_flag=True):\n",
    "    \"\"\"\n",
    "    Process a dataset of scenes using a neural network with the specified structure.\n",
    "\n",
    "    Args:\n",
    "    - doe: Neural network structure.\n",
    "    - v0_guess: Initial velocity guess.\n",
    "    - dataset: List of DataFrames containing scene data.\n",
    "    - processed_flag: Flag indicating whether the dataset has been processed.\n",
    "    - NUM_ITER: Number of iterations for training.\n",
    "    - LEARNING_RATE_v0: Learning rate for v0 optimization.\n",
    "\n",
    "    Returns:\n",
    "    - info_dataset: Concatenated DataFrame containing information about the evaluation for each scene.\n",
    "    \"\"\"\n",
    "    \n",
    "    tmp = []\n",
    "\n",
    "    print(\"--\"*50)\n",
    "    s = '-'.join(str(x) for x in doe)\n",
    "    print(f\"Analyzing {len(dataset)} dfs. NN structure: {s}\")\n",
    "    print(\"**\"*50)\n",
    "\n",
    "    model = create_model(doe)\n",
    "    \n",
    "    for step, df in enumerate(dataset):\n",
    "\n",
    "        print(f\"Analyzing {step+1}/{len(dataset)} dfs.\")\n",
    "        \n",
    "        info_df, model_trained = solve_nn_df(df, model, v0, NUM_ITER, LEARNING_RATE_v0, test, plot_flag)\n",
    "\n",
    "        # Store info about the NN structure\n",
    "        nrow = info_df.shape[0]\n",
    "        info_df['DOE'] = [doe]*nrow\n",
    "        info_df['processed'] = [processed_flag]*nrow\n",
    "\n",
    "        tmp.append(info_df)\n",
    "\n",
    "    # Concatenate individual DataFrames into a single DataFrame\n",
    "    info_dataset = pd.concat(tmp, sort=False, ignore_index=True)\n",
    "\n",
    "    return info_dataset, model_trained"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509d2d54",
   "metadata": {},
   "source": [
    "### Solve NN driven model in the whole dataset, looping over DOE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d0330f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def solve_nn_dataset_DOE(v0, dataset, processed_flag, NUM_ITER, LEARNING_RATE_v0, test):\n",
    "        \n",
    "#     DOE =[[1,2,1], [1,4,1], [1,10,1]] #Design of experiment\n",
    "#     tmp = []\n",
    "#     models_trained = []\n",
    "\n",
    "#     for DOE_struct in DOE:\n",
    "\n",
    "#         print(\"\\n\")\n",
    "#         print(\"==\"*30)\n",
    "\n",
    "#         s = '-'.join(str(x) for x in DOE_struct)\n",
    "#         print(f\"NN structure: {s}\")\n",
    "\n",
    "#         info_dataset, model_trained = solve_nn_dataset(dataset, DOE_struct, v0, processed_flag, NUM_ITER,\n",
    "#                                                        LEARNING_RATE_v0, test, plot_flag=True)\n",
    "\n",
    "#         # Store info about the NN structure\n",
    "#         nrow = info_dataset.shape[0]\n",
    "#         info_dataset['DOE'] = [DOE_struct]*nrow\n",
    "#         info_dataset['processed'] = [processed_flag]*nrow\n",
    "#         models_trained.append([model_trained, DOE_struct])\n",
    "\n",
    "        \n",
    "#         # Append info_dflist\n",
    "#         tmp.append(info_dataset)\n",
    "\n",
    "#         print(\"==\"*30)\n",
    "    \n",
    "#     # To better handling info_alldataset\n",
    "#     info_alldataset = pd.concat(tmp, sort=False);\n",
    "    \n",
    "#     return info_alldataset, models_trained"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
