{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d554e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ODE\n",
    "import numpy as np\n",
    "import pandas as pd # for data manipulation\n",
    "import time\n",
    "from scipy.integrate import odeint, solve_ivp\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import scipy.optimize as optimize\n",
    "\n",
    "from ipynb.fs.full.myfun_nn import *\n",
    "from ipynb.fs.full.myfun_model_usefulfuns import *\n",
    "from ipynb.fs.defs.myfun_plot import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d6b935",
   "metadata": {},
   "source": [
    "# NN driven model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd98145",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Traffic dynamic with ANN\n",
    "\n",
    "def TD_ANN_model(t, X, vel):\n",
    "    \n",
    "    \"\"\"\n",
    "    Lighthill-Whitham-Richards (LWR) traffic flow model in 1D\n",
    "    Transform a list as vel into a function of t,X.\n",
    "    \"\"\"\n",
    "    \n",
    "    d_x = vel\n",
    "        \n",
    "    return d_x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44fc198",
   "metadata": {},
   "source": [
    "## Ode solver for the NN driven model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c299620",
   "metadata": {},
   "outputs": [],
   "source": [
    "def odesolver_ann(x0, vel, t0, tend, deltat = 0.05):\n",
    "    \n",
    "    \"\"\"\n",
    "    odesolver_ann solves the TD_ANN_model ode system:\n",
    "    * in [t0, tend] with timestep as deltat,\n",
    "    * starting from x0\n",
    "    * vel is the rhs passed to TD_ANN_model to create the model\n",
    "    \"\"\"\n",
    "    \n",
    "    tspan_ann = time_discretization(t0,tend,deltat)\n",
    "        \n",
    "    sol_ann = odeint(TD_ANN_model, x0, tspan_ann, args=(vel,), tfirst = True).T\n",
    "\n",
    "    return tspan_ann, sol_ann"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eac22c3",
   "metadata": {},
   "source": [
    "### Default training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dcde325",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_ann_scene(scn):\n",
    "    \n",
    "    \"\"\"\n",
    "    create_data_ann_scene gives the X and y for an entire scene\n",
    "    \n",
    "    X_scn, y_scn = create_data_ann_scene(scn)\n",
    "    \n",
    "    X_scn is a list of consecutive distances btw the vehicles of a scene, at each timestamps\n",
    "    y_scn is a list of approximated velocities, of all the vehicles except the leader one.\n",
    "    \"\"\"\n",
    "\n",
    "    ## Create X\n",
    "    X_scn = scn['cons_dis']\n",
    "\n",
    "    ## Create y\n",
    "    dX_scn = np.diff(scn['Xarr'],axis=1)\n",
    "    dT_scn = np.diff(scn['Tarr'])\n",
    "    velocity = dX_scn/dT_scn # velocity at the timestamps\n",
    "\n",
    "    # we choose the first velocity discretized as (x_(i+1)-x_i)/deltaT\n",
    "    y_scn = velocity[:-1] #drop the last vehicle\n",
    "    \n",
    "    return X_scn, y_scn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55333fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_nn_scn_default(nn_model, scn, epochs, batch_size, v0, deltat=0.05, verbose=\"auto\"):\n",
    "    \n",
    "    \"\"\"\n",
    "    t_ann_list, x_ann_list, vel_ann_list = odesolver_ann_scene(nn_model, scn, epochs, batch_size, v0, deltat=0.05, verbose=\"auto\")\n",
    "    \n",
    "    odesolver_ann_scene solve the ode model driven by a nn in a scene.\n",
    "    \"\"\"\n",
    " \n",
    "    x_list, t_list, v_list = [[i] for i in scn['Xarr'][:,0]], [scn['Tarr'][0]], []\n",
    "    N, tstamps, fmt = scn['N. vehicles'], scn['Tarr'], '{0:.02f}'\n",
    "\n",
    "    X_arr, y_arr = create_data_ann_scene(scn)\n",
    "\n",
    "    print(\"==\"*50)\n",
    "    print(f\"We have {len(tstamps)-1} time intervals inside [{fmt.format(tstamps[0])},{fmt.format(tstamps[-1])}]\\n\")\n",
    "    \n",
    "    for i in range(0,len(tstamps)-1):\n",
    "\n",
    "        print(\"--\"*50)\n",
    "        print(f\"Time interval n.{i}: [{fmt.format(scn['Tarr'][i])}, {fmt.format(scn['Tarr'][i+1])}]\\n\")\n",
    "        \n",
    "        ## STEP 1: Train the NN model and predict the rhs of the TD_ANN_model\n",
    "        X, y = X_arr[:,i], y_arr[:,i]\n",
    "\n",
    "        train_nn(nn_model, X, y, epochs, batch_size, verbose)\n",
    "        y_pred = nn_model(X, training=True).numpy().flatten().tolist()\n",
    "\n",
    "        ## STEP 2: Solve the ODE sys in this time interval\n",
    "        x0 = [l[-1] for l in np.vstack(x_list).tolist()]  # last values computed\n",
    "        t0, tend = scn['Tarr'][i], scn['Tarr'][i+1]\n",
    "        v_ann = np.append(y_pred,v0).tolist()\n",
    "        tspan_ann, sol_ann = odesolver_ann(x0, v_ann, t0, tend, deltat=0.05)\n",
    "\n",
    "        print(f\"\\\n",
    "        * y_true: {y}\\n\\\n",
    "        * v_ann: {v_ann}\\n\")\n",
    "        \n",
    "        ## STEP 3: store info\n",
    "        x_list, t_list = update_sol_lists(N, tspan_ann, sol_ann, x_list, t_list)\n",
    "        v_list.append(v_ann)\n",
    "\n",
    "        print(\"--\"*50)\n",
    "    \n",
    "    print(\"==\"*50)\n",
    "    \n",
    "    return t_list, x_list, v_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36030428",
   "metadata": {},
   "source": [
    "### Custom training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7bd910",
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_step(model, scn, v0, it, PLOT_ITER, lists, nn_fun):\n",
    "\n",
    "    t_list, x_list, v_list = lists\n",
    "    X_arr, y_arr = create_data_ann_scene(scn)\n",
    "    N, tstamps, fmt = scn['N. vehicles'], scn['Tarr'], '{0:.02f}'\n",
    "\n",
    "    loss_fn, optimizer = nn_fun\n",
    "    \n",
    "    for i in range(0,len(tstamps)-1):\n",
    "                \n",
    "        # STEP 1: Create the dataset and train the nn model\n",
    "        X, y = X_arr[:,i], y_arr[:,i]\n",
    "        \n",
    "        ## Train the NN:  Update model coefs\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "\n",
    "            # Create tensor that you will watch\n",
    "            x_tensor = tf.convert_to_tensor(X, dtype=tf.float64)\n",
    "            tape.watch(x_tensor)\n",
    "\n",
    "            y_pred = model(X, training=True) # forward pass\n",
    "            loss_value = loss_fn(y_true = y, y_pred = y_pred) # loss function          \n",
    "\n",
    "        ## Compute gradients\n",
    "        trainable_vars = model.trainable_variables\n",
    "        grads = tape.gradient(loss_value, trainable_vars)\n",
    "        \n",
    "        ## Update weights\n",
    "        optimizer.apply_gradients(zip(grads, trainable_vars))\n",
    "        \n",
    "        # STEP 3: Solve the ODE sys in this time interval\n",
    "        x0 = [l[-1] for l in np.vstack(x_list).tolist()]  # last values computed\n",
    "        t0, tend = scn['Tarr'][i], scn['Tarr'][i+1]\n",
    "        v_ann = np.append(y_pred.numpy().flatten().tolist(),v0).tolist()\n",
    "        tspan_ann, sol_ann = odesolver_ann(x0, v_ann, t0, tend, deltat=0.05)    \n",
    "        \n",
    "        # STEP 4: Store the info\n",
    "        x_list, t_list = update_sol_lists(N, tspan_ann, sol_ann, x_list, t_list)\n",
    "        v_list.append(v_ann)\n",
    "                \n",
    "        if it % PLOT_ITER == 0:\n",
    "            print(f\"\\\n",
    "            - Time interval n.{i}: [{fmt.format(scn['Tarr'][i])}, {fmt.format(scn['Tarr'][i+1])}]\\n\\\n",
    "                * y_true: {y}\\n\\\n",
    "                * v_ann: {v_ann}\\n\")\n",
    "            print(\"--\"*50)\n",
    "        \n",
    "    return t_list, x_list, v_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1fa092",
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_nn_scn_custom(model, scn, v0, LEARNING_RATE_NN = 0.001, LEARNING_RATE_v0=0.5, NUM_ITER=200, PLOT_ITER=25):\n",
    "    \n",
    "    \"\"\"\n",
    "    t_list, x_list, v_list = solve_nn_scn_custom(model, scn, v0, LEARNING_RATE_NN)\n",
    "    \"\"\"\n",
    "    \n",
    "    N, tstamps, fmt = scn['N. vehicles'], scn['Tarr'], '{0:.02f}'\n",
    "    v0_scn = []\n",
    "    \n",
    "    print(\"==\"*50)\n",
    "    print(f\"\\\n",
    "    We have {len(tstamps)-1} time intervals inside [{fmt.format(tstamps[0])},{fmt.format(tstamps[-1])}]\")\n",
    "\n",
    "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "                    initial_learning_rate=LEARNING_RATE_NN,\n",
    "                    decay_steps=int(NUM_ITER/2)+1,\n",
    "                    decay_rate=0.9,\n",
    "                    staircase=True)\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.SGD(learning_rate=lr_schedule)\n",
    "    loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "    \n",
    "    err_list, err, diff = [], 1e9, 1\n",
    "    err_list.append(err)\n",
    "    \n",
    "    it = 1\n",
    "    err_best, it_best = err, it\n",
    "    \n",
    "    while (diff > 1e-6 and it<NUM_ITER+1):\n",
    "        \n",
    "        # STEP 1: Simulate the dynamic over a scene with v0\n",
    "        x_list, t_list, v_list = [[i] for i in scn['Xarr'][:,0]], [scn['Tarr'][0]], []\n",
    "        t_list, x_list, v_list = solve_step(model, scn, v0, it, PLOT_ITER,\n",
    "                                            lists = [t_list, x_list, v_list],\n",
    "                                            nn_fun = [loss_fn, optimizer])\n",
    "        _, sol_ann_matched = match_timestamps_scene(t_list, x_list)\n",
    "\n",
    "        # STEP 2: Update v0 with SGD\n",
    "        v0_upd, loss_val, grads, g = SGD_v0(scn, sol_ann_matched, v0, LEARNING_RATE_v0) \n",
    "        \n",
    "        ## Store v0 and update it\n",
    "        v0_scn.append(v0)\n",
    "        v0 = v0_upd\n",
    "        \n",
    "        err = loss_fn(y_true=scn['Xarr'], y_pred = sol_ann_matched).numpy()\n",
    "        err_list.append(err)\n",
    "\n",
    "        if err < err_best:\n",
    "            t_best, x_best, v_best = t_list, x_list, v_list\n",
    "            it_best, err_best = it, err\n",
    "        \n",
    "        #update diff\n",
    "        if it % 50 == 0:\n",
    "             diff = abs(err_list[-1]-err_list[-50])\n",
    "   \n",
    "        #print(f\"it={it}, err={err}\")\n",
    "        it += 1\n",
    "\n",
    "#     print(f\"it best = {it_best}, err = {err_best}\")\n",
    "    print(f\"\\\n",
    "    * err= {err}\\n\\\n",
    "    * Learning rate NN = {optimizer.learning_rate.numpy()}\\n\\\n",
    "    * diff = {diff}\")\n",
    "    \n",
    "    # plot function\n",
    "    tscale = 1+(tstamps[-1]-tstamps[0])/10000\n",
    "    title = f\"$df\\  n.\\ {scn['N. file']}\\ -\\ Scene\\ n.\\ {scn.name},\\ at\\ it={it-1}$\"\n",
    "    plot_scn(scn, sol_ann_matched, title, xbal=0.01, ybal=0.05, scale=tscale)\n",
    "    \n",
    "    return t_best, x_best, v_best, v0_scn, it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3e52f5",
   "metadata": {},
   "source": [
    "### Solve the nn-driven model in the all the scenes in a df, and get v0 mean for each scene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5245224a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_finder(model, scn, v0):\n",
    "    \n",
    "    \"\"\"\n",
    "    err_lr_best, lr_best, it_lr_best = lr_finder(model, scn, v0)\n",
    "    \"\"\"\n",
    "\n",
    "    X_arr, y_arr = create_data_ann_scene(scn)\n",
    "    lr_range = [0.01, 0.005, 0.001, 0.0005, 0.0001, 0.00005, 0.00001]\n",
    "    err_lr_best, lr_best = 1e9, None\n",
    "\n",
    "    for lr in lr_range:\n",
    "\n",
    "        mmodel = tf.keras.models.clone_model(model)\n",
    "        optimizer = tf.keras.optimizers.SGD(learning_rate=lr)\n",
    "        loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "        for it in range(25):\n",
    "            \n",
    "            x_list, t_list, v_list = [[i] for i in scn['Xarr'][:,0]], [scn['Tarr'][0]], []\n",
    "            t_list, x_list, v_list = solve_step(mmodel, scn, v0, -1, -2,\n",
    "                                                lists = [t_list, x_list, v_list],\n",
    "                                                nn_fun = [loss_fn, optimizer])\n",
    "            _, sol_ann_matched = match_timestamps_scene(t_list, x_list)            \n",
    "\n",
    "        err = loss_fn(y_true=scn['Xarr'], y_pred = sol_ann_matched).numpy()  \n",
    "\n",
    "#         print(f\"For lr={lr} we have err={err}\")\n",
    "        if err < err_lr_best:\n",
    "            err_lr_best, lr_best, it_lr_best = err, lr, it\n",
    "        \n",
    "#         print(f\"--> err_best={err_lr_best}, lr_best={lr_best}\\n\")\n",
    "\n",
    "    return err_lr_best, lr_best, it_lr_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538d271f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SGD_v0(scn, x_list_matched, v0, LEARNING_RATE_v0):\n",
    "    \n",
    "    \"\"\"\n",
    "    v0_upd, loss_val, grads, g = SGD_v0(scn, x_list_matched, v0, LEARNING_RATE_v0)\n",
    "    \"\"\"\n",
    "    \n",
    "    loss_objective = tf.keras.losses.MeanSquaredError()\n",
    "    \n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "\n",
    "        trajs_true_tensor = tf.convert_to_tensor(scn['Xarr'], dtype=tf.float64)\n",
    "\n",
    "        # Create tensor that you will watch\n",
    "        trajs_pred_tensor = tf.convert_to_tensor(x_list_matched, dtype=tf.float64)\n",
    "        tape.watch(trajs_pred_tensor)\n",
    "\n",
    "        loss_val = loss_objective(y_true = trajs_true_tensor, y_pred = trajs_pred_tensor)\n",
    "\n",
    "    # Compute gradients\n",
    "    grads = tape.gradient(loss_val, trajs_pred_tensor)\n",
    "\n",
    "    # Update weights\n",
    "    g = grads[-1].numpy()[1:].mean() # I'm watching at a mean velocity of the leader car..\n",
    "    v0_upd = v0 - LEARNING_RATE_v0*g\n",
    "    \n",
    "    return v0_upd, loss_val, grads, g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d32dbce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_nn_df(df, doe, v0, NUM_ITER, LEARNING_RATE_v0=0.5):\n",
    "    \n",
    "    \"\"\"\n",
    "    Solve nn in a single df ang get info_df, which gives info about v0_scn_mean for each scene.\n",
    "    \n",
    "    info_df = solve_nn_df(df, model, v0_guess=30,\n",
    "                          NUM_EPOCHS=10, LEARNING_RATE_NN=0.01, LEARNING_RATE_v0=0.5, flag_print=False)\n",
    "    \"\"\"\n",
    "    \n",
    "    scn_list = seq2scn(df)\n",
    "    info_scn, fmt = [], '{0:.02f}'\n",
    "    loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "    model = create_model(doe)\n",
    "    \n",
    "    for scn in scn_list:\n",
    "                \n",
    "        tstamps = scn['Tarr']\n",
    "        err_lr_best, lr_best, it_lr_best = lr_finder(model, scn, v0)\n",
    "       \n",
    "        print(f\"df n.{df['N. file'][0]}, scene n.{scn.name}/{len(scn_list)}\")\n",
    "    \n",
    "        t_list, x_list, v_list, v0_scn, it = solve_nn_scn_custom(model, scn, v0,\n",
    "                                                                    lr_best,\n",
    "                                                                    LEARNING_RATE_v0,\n",
    "                                                                    NUM_ITER, PLOT_ITER=NUM_ITER)\n",
    "\n",
    "        # Compute MAE for the solution computed\n",
    "        _, sol_ann_matched = match_timestamps_scene(t_list, x_list)\n",
    "        mae = loss_fn(y_true=scn['Xarr'], y_pred = sol_ann_matched).numpy()\n",
    "        \n",
    "        v0_scn_mean = np.array(v0_scn).mean()\n",
    "\n",
    "        info_scn.append([t_list, x_list, v_list, v0_scn, v0_scn_mean, scn.name, df['N. file'][0], it-1])\n",
    "\n",
    "        print(f\"\\\n",
    "        For scene {scn.name}/{len(scn_list)}\\n\\\n",
    "        * use LR_NN={lr_best} with err={err_lr_best} at it={it_lr_best}\\n\\\n",
    "        * v0_scn_mean = {v0_scn_mean}\\n\\\n",
    "        * MAE = {mae}\\n\")\n",
    "        print(\"==\"*50)\n",
    "        print(\"\\n\")\n",
    "\n",
    "    # to better handle, transposte info_df\n",
    "    tmp = list(map(list, zip(*info_scn)))\n",
    "\n",
    "    info_df = pd.DataFrame({'t_list': tmp[0],\\\n",
    "                            'x_list': tmp[1],\\\n",
    "                            'v_list': tmp[2],\\\n",
    "                            'v0_scn': tmp[3],\\\n",
    "                            'v0_scn_mean': tmp[4],\\\n",
    "                            'n_scn': tmp[5],\\\n",
    "                            'N. file': tmp[6],\\\n",
    "                            'iter': tmp[7]\n",
    "                           })\n",
    "  \n",
    "    return info_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1085ae",
   "metadata": {},
   "source": [
    "### Solve NN driven model in each df of a dataset, and get v0 mean for each scn in each df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2b6c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_nn_dataset(doe, v0_guess, dataset, processed_flag, NUM_ITER, LEARNING_RATE_v0=0.5):\n",
    "    \n",
    "    \"\"\"\n",
    "    info_dataset = solve_nn_dataset(doe, v0_guess, dataset, processed_flag, NUM_ITER, LEARNING_RATE_v0=0.5)\n",
    "    \"\"\"\n",
    "    \n",
    "    start_time2 = time.time()\n",
    "    tmp = []\n",
    "    \n",
    "    print(\"--\"*50)\n",
    "    s = '-'.join(str(x) for x in doe)\n",
    "    print(f\"NN structure: {s}\")\n",
    "    \n",
    "    for step, df in enumerate(dataset):\n",
    "        \n",
    "        start_time1 = time.time()\n",
    "        \n",
    "        print(\"**\"*50)\n",
    "        print(f\"In df n.{df['N. file'][0]}/{len(dataset)} we have {len(df)} scenes\")\n",
    "        \n",
    "        info_df = solve_nn_df(df, doe, v0_guess, NUM_ITER, LEARNING_RATE_v0)\n",
    "        \n",
    "        # Store info about the NN structure\n",
    "        nrow = info_df.shape[0]\n",
    "        info_df['DOE'] = [doe]*nrow\n",
    "        info_df['processed'] = [processed_flag]*nrow\n",
    "        \n",
    "        tmp.append(info_df)\n",
    "\n",
    "        print(f\"For df={df['N. file'][0]} with {len(df)} scenes, time taken:\\\n",
    "        {'{0:.02f}'.format(time.time() - start_time1)}\")\n",
    "        print(\"**\"*50)\n",
    "\n",
    "\n",
    "    time_taken=time.time() - start_time2\n",
    "    print(f\"\\nTime taken for the computation: {'{0:.02f}'.format(time_taken)}\")\n",
    "    print(\"--\"*50)\n",
    "\n",
    "    # To better handling info_dataset\n",
    "    info_dataset = pd.concat(tmp, sort=False)\n",
    "        \n",
    "    return info_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509d2d54",
   "metadata": {},
   "source": [
    "### Solve NN driven model in the whole dataset, looping over DOE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d0330f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def solve_nn_dataset_DOE(v0_guess, dataset, processed_flag, NUM_ITER, LEARNING_RATE_v0):\n",
    "    \n",
    "#     \"\"\"\n",
    "#     solve_nn_dataset_DOE(v0_guess, dataset, NUM_ITER, LEARNING_RATE_v0)\n",
    "#     \"\"\"\n",
    "    \n",
    "#     DOE =[[1,2,1], [1,4,1], [1,10,1]] #Design of experiment\n",
    "#     tmp = []\n",
    "\n",
    "#     start_time1 = time.time()\n",
    "#     for DOE_struct in DOE:\n",
    "\n",
    "#         print(\"\\n\")\n",
    "#         print(\"==\"*30)\n",
    "\n",
    "#         s = '-'.join(str(x) for x in DOE_struct)\n",
    "#         print(f\"NN structure: {s}\")\n",
    "\n",
    "#         start_time2 = time.time()\n",
    "\n",
    "#         info_dataset = solve_nn_dataset(DOE_struct, v0_guess, dataset, NUM_ITER, LEARNING_RATE_v0)\n",
    "\n",
    "#         # Store info about the NN structure\n",
    "#         nrow = info_dataset.shape[0]\n",
    "#         info_dataset['DOE'] = [DOE_struct]*nrow\n",
    "#         info_dataset['processed'] = [processed_flag]*nrow\n",
    "\n",
    "#         # Append info_dflist\n",
    "#         tmp.append(info_dataset)\n",
    "\n",
    "#         time_taken=time.time() - start_time2\n",
    "#         print(f\"\\nTime taken using {s} NN structure: {'{0:.02f}'.format(time_taken)}\")\n",
    "#         print(\"==\"*30)\n",
    "\n",
    "#     time_taken=time.time() - start_time1\n",
    "#     print(f\"\\nTime taken for the computation: {'{0:.02f}'.format(time_taken)}\")\n",
    "    \n",
    "#     # To better handling info_alldataset\n",
    "#     info_alldataset = pd.concat(tmp, sort=False);\n",
    "    \n",
    "#     return info_alldataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
