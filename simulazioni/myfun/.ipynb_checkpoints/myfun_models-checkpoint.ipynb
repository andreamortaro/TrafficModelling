{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d554e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ODE\n",
    "import numpy as np\n",
    "import pandas as pd # for data manipulation\n",
    "import time\n",
    "from scipy.integrate import odeint, solve_ivp\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import scipy.optimize as optimize\n",
    "\n",
    "from ipynb.fs.full.myfun_nn import *\n",
    "from ipynb.fs.defs.myfun_plot import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27b63b7",
   "metadata": {},
   "source": [
    "# Define the LWR models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ddce756",
   "metadata": {},
   "source": [
    "## Lin and Log models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6718d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Traffic dynamic with LWR-model\n",
    "\n",
    "def TD_LWR_model(t, X, N, v0, L, flag):\n",
    "    \"\"\"\n",
    "    Lighthill-Whitham-Richards (LWR) traffic flow model in 1D\n",
    "    \"\"\"\n",
    "    #N, v0, L, flag = args[0], args[1], args[2], args[3]\n",
    "    \n",
    "    # W function\n",
    "    match flag:\n",
    "        case \"Lin\":\n",
    "            W = lambda z: v0*(1-1/z)\n",
    "        case \"Log\":\n",
    "            W = lambda z: v0*np.log(z)\n",
    "        case _:\n",
    "            return f\"No match for {flag}, you can only choose between \\\"Lin\\\" and \\\"Log\\\"\"\n",
    "\n",
    "    # ode sys\n",
    "    d_x = np.zeros(N)\n",
    "    \n",
    "    for i in range(0,N-1):\n",
    "        tmp = (X[i+1] - X[i])/L\n",
    "        d_x[i] = W(tmp)\n",
    "\n",
    "    d_x[N-1] = v0\n",
    "        \n",
    "    return d_x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a53591c",
   "metadata": {},
   "source": [
    "### Improve v0 and L in a scene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72d8c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_loss(TD_LWR_model, flag, scn, v0, L, whole_tspan, x0, N, deltat):\n",
    "\n",
    "    \"\"\"\n",
    "    f, loss_fun = define_loss(TD_LWR_model, flag, v0, L, whole_tspan, x0, N)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Solve scn, freezing v0 and L\n",
    "    def f(params):\n",
    "        v0, L = params\n",
    "        sol = odeint(TD_LWR_model, x0, whole_tspan, args=(N, v0, L, flag), tfirst = True).T  \n",
    "        return sol\n",
    "\n",
    "    # Define Loss function to be optimized\n",
    "    def loss_fun(params):\n",
    "\n",
    "        sol = f(params)\n",
    "        _, sol_matched = match_timestamps_scene(whole_tspan, sol, deltat)\n",
    "        return mean_squared_error(y_true = scn['Xarr'], y_pred = sol_matched)\n",
    "    \n",
    "    return f, loss_fun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50e7447e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_v0_L(TD_LWR_model, flag, scn, v0, L, deltat=0.05, tol=1e-10):\n",
    "    \n",
    "    N, tstamps = scn['N. vehicles'], scn['Tarr']\n",
    "    whole_tspan = time_discretization(tstamps[0], tstamps[-1], deltat)\n",
    "    x0 = scn['Xarr'][:,0].tolist()\n",
    "    \n",
    "    f, loss_fun = define_loss(TD_LWR_model, flag, scn, v0, L, whole_tspan, x0, N, deltat)\n",
    "\n",
    "    print('--'*50)\n",
    "    print(f\"Scene n.{scn.name}.\\n\\\n",
    "    Initial mse: {loss_fun([v0,L])}\\n\")\n",
    "\n",
    "    # Optimizing procedure\n",
    "    bnds = ((0, np.inf), (2, np.inf))\n",
    "    result = optimize.minimize(loss_fun, x0 = [v0,L], bounds = bnds, method=\"L-BFGS-B\",tol=tol)\n",
    "    if result.success:\n",
    "        v0_upd, L_upd = result.x\n",
    "        print(f\"After optimizing, new params: v0 = {v0_upd}, L = {L_upd}\")\n",
    "    else:\n",
    "#          raise ValueError(result.message)\n",
    "        v0_upd, L_upd = v0, L\n",
    "        print(\"Optimization not succeed\")\n",
    "    \n",
    "\n",
    "    print(f\"\\nInitial mse: {loss_fun([v0_upd, L_upd])}\")\n",
    "    print('--'*50)\n",
    "    \n",
    "    return v0_upd, L_upd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1623d3b7",
   "metadata": {},
   "source": [
    "### Ode solver LIN/LOG model in a scn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb616bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_TD_LWR_scn(TD_LWR_model, flag, scn, v0, L, deltat, pplot=False):\n",
    "\n",
    "    \"\"\"\n",
    "    x_list, t_list = solve_TD_LWR_scn(TD_LWR_model, flag, scn, v0, L, deltat)\n",
    "    \"\"\"\n",
    "    \n",
    "    N, tstamps, fmt = scn['N. vehicles'], scn['Tarr'], '{0:.02f}'\n",
    "    \n",
    "    print(\"--\"*50)\n",
    "    print(f\"We have {len(tstamps)-1} time intervals inside \\\n",
    "    [{fmt.format(tstamps[0])},{fmt.format(tstamps[-1])}]\")\n",
    "\n",
    "    x_list, t_list, v_list = [[i] for i in scn['Xarr'][:,0]], [scn['Tarr'][0]], []\n",
    "\n",
    "    for i in range(0,len(tstamps)-1):\n",
    "\n",
    "        print(f\"Time interval n.{i}: [{fmt.format(scn['Tarr'][i])}, {fmt.format(scn['Tarr'][i+1])}]\")\n",
    "\n",
    "        ## STEP 1: Solve the ODE sys in this time interval\n",
    "        x0 = [l[-1] for l in np.vstack(x_list).tolist()]  # last values computed\n",
    "        t0, tend = scn['Tarr'][i], scn['Tarr'][i+1]\n",
    "        tspan = time_discretization(t0, tend, deltat=0.05)\n",
    "\n",
    "        sol = odeint(TD_LWR_model, x0, tspan, args=(N, v0, L, flag), tfirst = True).T # take the transpose to get N trajs!\n",
    "\n",
    "        ## STEP 2: store info\n",
    "        x_list, t_list = update_sol_lists(N, tspan, sol, x_list, t_list)\n",
    "\n",
    "        ## STEP 3: Plot the result: with scn['Xarr']\n",
    "        if pplot:\n",
    "            _, x_list_matched = match_timestamps_scene(t_list, x_list)\n",
    "            title = rf\"$TD\\_LWR\\_model\\ using\\ W\\_{flag}$\"\n",
    "            plot_TD_LWR_scn(scn, x_list_matched, title)\n",
    "\n",
    "    print(\"--\"*50)\n",
    "    \n",
    "    return x_list, t_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a66788",
   "metadata": {},
   "source": [
    "### LIN/LOG model in a df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5f4a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_TD_LWR_df(df, TD_LWR_model, flag, v0=30, L=5, deltat=0.05, tol=1e-8, pplot=False):\n",
    "\n",
    "    info_scn = []\n",
    "    \n",
    "    for _, scn in df.iterrows():\n",
    "\n",
    "        tstamps, fmt = scn['Tarr'], '{0:.02f}'\n",
    "\n",
    "        print(\"==\"*50)\n",
    "        print(f\"df n.{df['N. file'][0]} - Scene n.{scn.name}/{len(df)}\")\n",
    "\n",
    "        # Optimize v0 and L\n",
    "        v0_upd, L_upd = optimize_v0_L(TD_LWR_model, flag, scn, v0=30, L=5, deltat=deltat, tol=1e-8)\n",
    "\n",
    "        # Solve the ODE\n",
    "        x_list, t_list = solve_TD_LWR_scn(TD_LWR_model, flag, scn, v0_upd, L_upd, deltat, pplot=False)\n",
    "        \n",
    "        # Store the result\n",
    "        info_scn.append([t_list, x_list, v0_upd, L_upd, scn.name, df['N. file'][0]])\n",
    "\n",
    "        print(\"==\"*50)   \n",
    "\n",
    "    # to better handle, transposte info_df\n",
    "    tmp = list(map(list, zip(*info_scn)))\n",
    "\n",
    "    info_df = pd.DataFrame({'t_list': tmp[0],\\\n",
    "                            'x_list': tmp[1],\\\n",
    "                            'v0_scn': tmp[2],\\\n",
    "                            'L_scn': tmp[3],\\\n",
    "                            'n_scn': tmp[4],\\\n",
    "                            'N. file': tmp[5]\n",
    "                           })\n",
    "  \n",
    "    return info_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24d756e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_TD_LWR_df_flag(df, TD_LWR_model, v0=30, L=5, deltat=0.05, tol=1e-8, pplot=False):\n",
    "    \n",
    "    \"\"\"\n",
    "    info_df = solve_TD_LWR_df_flag(df, TD_LWR_model, v0=30, L=5, deltat=0.05, tol=1e-8, pplot=False)\n",
    "    \"\"\"\n",
    "    \n",
    "    l = []\n",
    "    for flag in ['Lin', 'Log']:\n",
    "\n",
    "        tmp = solve_TD_LWR_df(df, TD_LWR_model, flag, v0=30, L=5, deltat=0.05, tol=1e-12, pplot=False)\n",
    "        tmp['LWR_flag'] = [flag] * tmp.shape[0]\n",
    "\n",
    "        l.append(tmp)\n",
    "        \n",
    "    info_df = pd.concat(l)\n",
    "    \n",
    "    return info_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae9a80b",
   "metadata": {},
   "source": [
    "### LIN/LOG model in all the df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84774eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_TD_LWR_dataset(dflist, TD_LWR_model, v0=30, L=5, deltat=0.05, tol=1e-8, pplot=False):\n",
    "\n",
    "    l = []\n",
    "    for df in dflist:\n",
    "        tmp = solve_TD_LWR_df_flag(df, TD_LWR_model, v0=30, L=5, deltat=0.05, tol=1e-8, pplot=False)    \n",
    "        l.append(tmp)\n",
    "        \n",
    "    info_alldataset = pd.concat(l)\n",
    "    \n",
    "    return info_alldataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d6b935",
   "metadata": {},
   "source": [
    "## NN driven model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd98145",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Traffic dynamic with ANN\n",
    "\n",
    "def TD_ANN_model(t, X, vel):\n",
    "    \n",
    "    \"\"\"\n",
    "    Lighthill-Whitham-Richards (LWR) traffic flow model in 1D\n",
    "    Transform a list as vel into a function of t,X.\n",
    "    \"\"\"\n",
    "    \n",
    "    d_x = vel\n",
    "        \n",
    "    return d_x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44fc198",
   "metadata": {},
   "source": [
    "### Ode solver for the NN driven model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62becfdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_discretization(t0, tend, deltat=0.05):\n",
    "    \n",
    "    Nt = round((tend-t0)/deltat) + 1               # number of discretization points\n",
    "                                                   # cast the value into int, us round to avoid cast problem\n",
    "    tspan = np.linspace(t0, tend, int(Nt))         # timespan\n",
    "    \n",
    "    return tspan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c299620",
   "metadata": {},
   "outputs": [],
   "source": [
    "def odesolver_ann(x0, vel, t0, tend, deltat = 0.05):\n",
    "    \n",
    "    \"\"\"\n",
    "    odesolver_ann solves the TD_ANN_model ode system:\n",
    "    * in [t0, tend] with timestep as deltat,\n",
    "    * starting from x0\n",
    "    * vel is the rhs passed to TD_ANN_model to create the model\n",
    "    \"\"\"\n",
    "    \n",
    "    tspan_ann = time_discretization(t0,tend,deltat)\n",
    "        \n",
    "    sol_ann = odeint(TD_ANN_model, x0, tspan_ann, args=(vel,), tfirst = True).T\n",
    "\n",
    "    return tspan_ann, sol_ann"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00022eda",
   "metadata": {},
   "source": [
    "### Useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dcde325",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_ann_scene(scn):\n",
    "    \n",
    "    \"\"\"\n",
    "    create_data_ann_scene gives the X and y for an entire scene\n",
    "    \n",
    "    X_scn, y_scn = create_data_ann_scene(scn)\n",
    "    \n",
    "    X_scn is a list of consecutive distances btw the vehicles of a scene, at each timestamps\n",
    "    y_scn is a list of approximated velocities, of all the vehicles except the leader one.\n",
    "    \"\"\"\n",
    "\n",
    "    ## Create X\n",
    "    X_scn = scn['cons_dis']\n",
    "\n",
    "    ## Create y\n",
    "    dX_scn = np.diff(scn['Xarr'],axis=1)\n",
    "    dT_scn = np.diff(scn['Tarr'])\n",
    "    velocity = dX_scn/dT_scn # velocity at the timestamps\n",
    "\n",
    "    # we choose the first velocity discretized as (x_(i+1)-x_i)/deltaT\n",
    "    y_scn = velocity[:-1] #drop the last vehicle\n",
    "    \n",
    "    return X_scn, y_scn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1323fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq2scn(df):\n",
    "    \n",
    "    \"\"\"\n",
    "    get an array of scenes, pandas obj\n",
    "    \"\"\"\n",
    "    \n",
    "    seq = []\n",
    "    \n",
    "    # extract input and target for each scene\n",
    "    for row in df.iterrows(): #run over rows\n",
    "        scn = row[1]\n",
    "        seq.append(scn)\n",
    "\n",
    "    return seq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04749ec5",
   "metadata": {},
   "source": [
    "### Solve the NN-driven model in a scene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80cbf531",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_timestamps_scene(t, x, deltat = 0.05):\n",
    "    \n",
    "    \"\"\"\n",
    "    Match the computed solution to the same timestamps of the scene\n",
    "    \n",
    "    t_matched, x_matched = match_timestamps_scene(t, x, deltat = 0.05)\n",
    "    \"\"\"\n",
    "    # To recover the same timestep in the data\n",
    "    factor = int(0.2/deltat)\n",
    "    \n",
    "    t_matched = np.array(t)[::factor]\n",
    "    x_matched = np.array([traj[::factor] for traj in x])\n",
    "    \n",
    "    return t_matched, x_matched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec84188",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_sol_lists(N, tspan_ann, sol_ann, x_list, t_list):\n",
    "    \n",
    "    \"\"\"\n",
    "    Once you solve the ode in a sub-interval of a scene, you update the lists containing t,x\n",
    "    \n",
    "    x_list, t_list = update_sol_lists(N, tspan_ann, sol_ann, x_list, t_list)\n",
    "    \"\"\"\n",
    "    \n",
    "    x_ann = sol_ann.tolist()\n",
    "    t_ann = tspan_ann[1:] # avoid the first recording\n",
    "\n",
    "    # add sol to the correct veh\n",
    "    for j in range(0,N):\n",
    "        tmp = x_ann[j][1:] # avoid the first recording\n",
    "        x_list[j] = np.concatenate([x_list[j],tmp])\n",
    "    t_list = np.concatenate([t_list,t_ann]).tolist()\n",
    "    \n",
    "    return x_list, t_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eac22c3",
   "metadata": {},
   "source": [
    "### Default training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55333fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_nn_scn_default(nn_model, scn, epochs, batch_size, v0, deltat=0.05, verbose=\"auto\"):\n",
    "    \n",
    "    \"\"\"\n",
    "    t_ann_list, x_ann_list, vel_ann_list = odesolver_ann_scene(nn_model, scn, epochs, batch_size, v0, deltat=0.05, verbose=\"auto\")\n",
    "    \n",
    "    odesolver_ann_scene solve the ode model driven by a nn in a scene.\n",
    "    \"\"\"\n",
    " \n",
    "    x_list, t_list, v_list = [[i] for i in scn['Xarr'][:,0]], [scn['Tarr'][0]], []\n",
    "    N, tstamps, fmt = scn['N. vehicles'], scn['Tarr'], '{0:.02f}'\n",
    "\n",
    "    X_arr, y_arr = create_data_ann_scene(scn)\n",
    "\n",
    "    print(\"==\"*50)\n",
    "    print(f\"We have {len(tstamps)-1} time intervals inside [{fmt.format(tstamps[0])},{fmt.format(tstamps[-1])}]\\n\")\n",
    "    \n",
    "    for i in range(0,len(tstamps)-1):\n",
    "\n",
    "        print(\"--\"*50)\n",
    "        print(f\"Time interval n.{i}: [{fmt.format(scn['Tarr'][i])}, {fmt.format(scn['Tarr'][i+1])}]\\n\")\n",
    "        \n",
    "        ## STEP 1: Train the NN model and predict the rhs of the TD_ANN_model\n",
    "        X, y = X_arr[:,i], y_arr[:,i]\n",
    "\n",
    "        train_nn(nn_model, X, y, epochs, batch_size, verbose)\n",
    "        y_pred = nn_model(X, training=True).numpy().flatten().tolist()\n",
    "\n",
    "        ## STEP 2: Solve the ODE sys in this time interval\n",
    "        x0 = [l[-1] for l in np.vstack(x_list).tolist()]  # last values computed\n",
    "        t0, tend = scn['Tarr'][i], scn['Tarr'][i+1]\n",
    "        v_ann = np.append(y_pred,v0).tolist()\n",
    "        tspan_ann, sol_ann = odesolver_ann(x0, v_ann, t0, tend, deltat=0.05)\n",
    "\n",
    "        print(f\"\\\n",
    "        * y_true: {y}\\n\\\n",
    "        * v_ann: {v_ann}\\n\")\n",
    "        \n",
    "        ## STEP 3: store info\n",
    "        x_list, t_list = update_sol_lists(N, tspan_ann, sol_ann, x_list, t_list)\n",
    "        v_list.append(v_ann)\n",
    "\n",
    "        print(\"--\"*50)\n",
    "    \n",
    "    print(\"==\"*50)\n",
    "    \n",
    "    return t_list, x_list, v_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36030428",
   "metadata": {},
   "source": [
    "### Custom training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7bd910",
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_step(model, scn, v0, it, PLOT_ITER, lists, nn_fun):\n",
    "\n",
    "    t_list, x_list, v_list = lists\n",
    "    X_arr, y_arr = create_data_ann_scene(scn)\n",
    "    N, tstamps, fmt = scn['N. vehicles'], scn['Tarr'], '{0:.02f}'\n",
    "\n",
    "    loss_fn, optimizer = nn_fun\n",
    "    \n",
    "    for i in range(0,len(tstamps)-1):\n",
    "                \n",
    "        # STEP 1: Create the dataset and train the nn model\n",
    "        X, y = X_arr[:,i], y_arr[:,i]\n",
    "        \n",
    "        ## Train the NN:  Update model coefs\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "\n",
    "            # Create tensor that you will watch\n",
    "            x_tensor = tf.convert_to_tensor(X, dtype=tf.float64)\n",
    "            tape.watch(x_tensor)\n",
    "\n",
    "            y_pred = model(X, training=True) # forward pass\n",
    "            loss_value = loss_fn(y_true = y, y_pred = y_pred) # loss function          \n",
    "\n",
    "        ## Compute gradients\n",
    "        trainable_vars = model.trainable_variables\n",
    "        grads = tape.gradient(loss_value, trainable_vars)\n",
    "        \n",
    "        ## Update weights\n",
    "        optimizer.apply_gradients(zip(grads, trainable_vars))\n",
    "        \n",
    "        # STEP 3: Solve the ODE sys in this time interval\n",
    "        x0 = [l[-1] for l in np.vstack(x_list).tolist()]  # last values computed\n",
    "        t0, tend = scn['Tarr'][i], scn['Tarr'][i+1]\n",
    "        v_ann = np.append(y_pred.numpy().flatten().tolist(),v0).tolist()\n",
    "        tspan_ann, sol_ann = odesolver_ann(x0, v_ann, t0, tend, deltat=0.05)    \n",
    "        \n",
    "        # STEP 4: Store the info\n",
    "        x_list, t_list = update_sol_lists(N, tspan_ann, sol_ann, x_list, t_list)\n",
    "        v_list.append(v_ann)\n",
    "                \n",
    "        if it % PLOT_ITER == 0:\n",
    "            print(f\"\\\n",
    "            - Time interval n.{i}: [{fmt.format(scn['Tarr'][i])}, {fmt.format(scn['Tarr'][i+1])}]\\n\\\n",
    "                * y_true: {y}\\n\\\n",
    "                * v_ann: {v_ann}\\n\")\n",
    "            print(\"--\"*50)\n",
    "        \n",
    "    return t_list, x_list, v_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1fa092",
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_nn_scn_custom(model, scn, v0, LEARNING_RATE_NN = 0.001, LEARNING_RATE_v0=0.5, NUM_ITER=200, PLOT_ITER=25):\n",
    "    \n",
    "    \"\"\"\n",
    "    t_list, x_list, v_list = solve_nn_scn_custom(model, scn, v0, LEARNING_RATE_NN)\n",
    "    \"\"\"\n",
    "    \n",
    "    N, tstamps, fmt = scn['N. vehicles'], scn['Tarr'], '{0:.02f}'\n",
    "    v0_scn = []\n",
    "    \n",
    "#     print(\"==\"*50)\n",
    "#     print(f\"\\\n",
    "#     We have {len(tstamps)-1} time intervals inside [{fmt.format(tstamps[0])},{fmt.format(tstamps[-1])}]\")\n",
    "\n",
    "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "                    initial_learning_rate=LEARNING_RATE_NN,\n",
    "                    decay_steps=int(NUM_ITER/2)+1,\n",
    "                    decay_rate=0.9,\n",
    "                    staircase=True)\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.SGD(learning_rate=lr_schedule)\n",
    "    loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "    \n",
    "    err_list, err, diff = [], 1e9, 1\n",
    "    err_list.append(err)\n",
    "    \n",
    "    it = 1\n",
    "    err_best, it_best = err, it\n",
    "    \n",
    "    while (diff > 1e-6 and it<NUM_ITER+1):\n",
    "        \n",
    "        # STEP 1: Simulate the dynamic over a scene with v0\n",
    "        x_list, t_list, v_list = [[i] for i in scn['Xarr'][:,0]], [scn['Tarr'][0]], []\n",
    "        t_list, x_list, v_list = solve_step(model, scn, v0, it, PLOT_ITER,\n",
    "                                            lists = [t_list, x_list, v_list],\n",
    "                                            nn_fun = [loss_fn, optimizer])\n",
    "        _, sol_ann_matched = match_timestamps_scene(t_list, x_list)\n",
    "\n",
    "        # STEP 2: Update v0 with SGD\n",
    "        v0_upd, loss_val, grads, g = SGD_v0(scn, sol_ann_matched, v0, LEARNING_RATE_v0) \n",
    "        \n",
    "        ## Store v0 and update it\n",
    "        v0_scn.append(v0)\n",
    "        v0 = v0_upd\n",
    "        \n",
    "        err = loss_fn(y_true=scn['Xarr'], y_pred = sol_ann_matched).numpy()\n",
    "        err_list.append(err)\n",
    "\n",
    "        if err < err_best:\n",
    "            t_best, x_best, v_best = t_list, x_list, v_list\n",
    "            it_best, err_best = it, err\n",
    "        \n",
    "        #update diff\n",
    "        if it % 10 == 0:\n",
    "             diff = abs(err_list[-1]-err_list[-10])\n",
    "\n",
    "        if it % PLOT_ITER == 0:\n",
    "            print(f\"\\\n",
    "            * err= {err}\\n\\\n",
    "            * Learning rate NN = {optimizer.learning_rate.numpy()}\\n\\\n",
    "            * diff = {diff}\")   \n",
    "\n",
    "            # plot function\n",
    "            tscale = 1+(tstamps[-1]-tstamps[0])/10000\n",
    "            title = f\"$df\\  n.\\ {scn['N. file']}\\ -\\ Scene\\ n.\\ {scn.name},\\ at\\ it={it}$\"\n",
    "            plot_scn(scn, sol_ann_matched, title, xbal=0.01, ybal=0.05, scale=tscale)\n",
    "            \n",
    "#             print(\"==\"*50)\n",
    "   \n",
    "        #print(f\"it={it}, err={err}\")\n",
    "        it += 1\n",
    "\n",
    "#     print(f\"it best = {it_best}, err = {err_best}\")\n",
    "    \n",
    "#     # plot function\n",
    "#     _, sol_ann_matched = match_timestamps_scene(t_best, x_best)\n",
    "#     tscale = 1+(tstamps[-1]-tstamps[0])/10000\n",
    "#     title = f\"$Scene\\ n.\\ {scn.name},\\ at\\ it\\_best={it_best}$\"\n",
    "#     plot_scn(scn, sol_ann_matched, title, xbal=0.01, ybal=0.05, scale=tscale)\n",
    "#     print(\"==\"*50)\n",
    "    \n",
    "    return t_best, x_best, v_best, v0_scn, it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3e52f5",
   "metadata": {},
   "source": [
    "### Solve the nn-driven model in the all the scenes in a df, and get v0 mean for each scene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5245224a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_finder(model, scn, v0):\n",
    "    \n",
    "    \"\"\"\n",
    "    lr_best = lr_finder(model)\n",
    "    \"\"\"\n",
    "\n",
    "    X_arr, y_arr = create_data_ann_scene(scn)\n",
    "    lr_range = [0.01, 0.005, 0.001, 0.0005, 0.0001, 0.00005, 0.00001]\n",
    "    err_lr_best, lr_best = 1e9, None\n",
    "\n",
    "    for lr in lr_range:\n",
    "\n",
    "        mmodel = tf.keras.models.clone_model(model)\n",
    "        optimizer = tf.keras.optimizers.SGD(learning_rate=lr)\n",
    "        loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "        for it in range(20):\n",
    "            \n",
    "            x_list, t_list, v_list = [[i] for i in scn['Xarr'][:,0]], [scn['Tarr'][0]], []\n",
    "            t_list, x_list, v_list = solve_step(mmodel, scn, v0, -1, -2,\n",
    "                                                lists = [t_list, x_list, v_list],\n",
    "                                                nn_fun = [loss_fn, optimizer])\n",
    "            _, sol_ann_matched = match_timestamps_scene(t_list, x_list)            \n",
    "\n",
    "        err = loss_fn(y_true=scn['Xarr'], y_pred = sol_ann_matched).numpy()  \n",
    "\n",
    "#         print(f\"For lr={lr} we have err={err}\")\n",
    "        if err < err_lr_best:\n",
    "            err_lr_best, lr_best, it_lr_best = err, lr, it\n",
    "        \n",
    "#         print(f\"--> err_best={err_lr_best}, lr_best={lr_best}\\n\")\n",
    "\n",
    "    return err_lr_best, lr_best, it_lr_best "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538d271f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SGD_v0(scn, x_list_matched, v0, LEARNING_RATE_v0):\n",
    "    \n",
    "    \"\"\"\n",
    "    v0_upd, loss_val, grads, g = SGD_v0(scn, x_list_matched, v0, LEARNING_RATE_v0)\n",
    "    \"\"\"\n",
    "    \n",
    "    loss_objective = tf.keras.losses.MeanSquaredError()\n",
    "    \n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "\n",
    "        trajs_true_tensor = tf.convert_to_tensor(scn['Xarr'], dtype=tf.float64)\n",
    "\n",
    "        # Create tensor that you will watch\n",
    "        trajs_pred_tensor = tf.convert_to_tensor(x_list_matched, dtype=tf.float64)\n",
    "        tape.watch(trajs_pred_tensor)\n",
    "\n",
    "        loss_val = loss_objective(y_true = trajs_true_tensor, y_pred = trajs_pred_tensor)\n",
    "\n",
    "    # Compute gradients\n",
    "    grads = tape.gradient(loss_val, trajs_pred_tensor)\n",
    "\n",
    "    # Update weights\n",
    "    g = grads[-1].numpy()[1:].mean() # I'm watching at a mean velocity of the leader car..\n",
    "    v0_upd = v0 - LEARNING_RATE_v0*g\n",
    "    \n",
    "    return v0_upd, loss_val, grads, g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d32dbce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_nn_df(df, model, v0, NUM_ITER, LEARNING_RATE_v0=0.5):\n",
    "    \n",
    "    \"\"\"\n",
    "    Solve nn in a single df ang get info_df, which gives info about v0_scn_mean for each scene.\n",
    "    \n",
    "    info_df = solve_nn_df(df, model, v0_guess=30,\n",
    "                          NUM_EPOCHS=10, LEARNING_RATE_NN=0.01, LEARNING_RATE_v0=0.5, flag_print=False)\n",
    "    \"\"\"\n",
    "    \n",
    "    scn_list = seq2scn(df)\n",
    "    info_scn, fmt = [], '{0:.02f}'\n",
    "    loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "    \n",
    "    for scn in scn_list:\n",
    "                \n",
    "        tstamps = scn['Tarr']\n",
    "        err_lr_best, lr_best, it_lr_best = lr_finder(model, scn, v0)\n",
    "       \n",
    "        t_list, x_list, v_list, v0_scn, it = solve_nn_scn_custom(model, scn, v0,\n",
    "                                                                    lr_best,\n",
    "                                                                    LEARNING_RATE_v0,\n",
    "                                                                    NUM_ITER, PLOT_ITER=NUM_ITER)\n",
    "\n",
    "        # Compute MAE for the solution computed\n",
    "        _, sol_ann_matched = match_timestamps_scene(t_list, x_list)\n",
    "        mae = loss_fn(y_true=scn['Xarr'], y_pred = sol_ann_matched).numpy()\n",
    "        \n",
    "        v0_scn_mean = np.array(v0_scn).mean()\n",
    "\n",
    "        info_scn.append([t_list, x_list, v_list, v0_scn, v0_scn_mean, scn.name, df['N. file'][0], it-1])\n",
    "\n",
    "        print(f\"\\\n",
    "        For scene {scn.name}/{len(scn_list)}\\n\\\n",
    "        * use LR_NN={lr_best} with err={err_lr_best} at it={it_lr_best}\\n\\\n",
    "        * v0_scn_mean = {v0_scn_mean}\\n\\\n",
    "        * MAE = {mae}\\n\")\n",
    "        print(\"==\"*50)\n",
    "        print(\"\\n\")\n",
    "\n",
    "    # to better handle, transposte info_df\n",
    "    tmp = list(map(list, zip(*info_scn)))\n",
    "\n",
    "    info_df = pd.DataFrame({'t_list': tmp[0],\\\n",
    "                            'x_list': tmp[1],\\\n",
    "                            'v_list': tmp[2],\\\n",
    "                            'v0_scn': tmp[3],\\\n",
    "                            'v0_scn_mean': tmp[4],\\\n",
    "                            'n_scn': tmp[5],\\\n",
    "                            'N. file': tmp[6],\\\n",
    "                            'iter': tmp[7]\n",
    "                           })\n",
    "  \n",
    "    return info_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1085ae",
   "metadata": {},
   "source": [
    "### Solve NN driven model in each df of a dataset, and get v0 mean for each scn in each df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2b6c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_nn_dataset(model, v0_guess, dataset, NUM_ITER=50, LEARNING_RATE_v0=0.5):\n",
    "    \n",
    "    \"\"\"\n",
    "    v0_dflist = v0_dataset(model, v0_guess, dataset, NUM_EPOCHS, LEARNING_RATE_NN, LEARNING_RATE_v0, flag_print)\n",
    "    \n",
    "    For each df in dataset compute v0 for each scene in df. Return v0_dflist.\n",
    "    \"\"\"\n",
    "    \n",
    "    start_time2 = time.time()\n",
    "    tmp = []\n",
    "    \n",
    "    for step, df in enumerate(dataset):\n",
    "        \n",
    "        start_time1 = time.time()\n",
    "\n",
    "        print(\"==\"*30)\n",
    "        \n",
    "        print(\"**\"*50)\n",
    "        print(f\"In df n.{df['N. file'][0]}/{len(dataset)} we have {len(df)} scenes\")\n",
    "        print(\"**\"*50)\n",
    "        \n",
    "        info_df = solve_nn_df(df, model, v0_guess, NUM_ITER, LEARNING_RATE_v0)\n",
    "\n",
    "        tmp.append(info_df)\n",
    "\n",
    "        print(f\"For df={df['N. file'][0]} with {len(df)} scenes, time taken:\\\n",
    "        {'{0:.02f}'.format(time.time() - start_time1)}\")\n",
    "        print(\"==\"*30)\n",
    "\n",
    "        print(\"--\"*30)\n",
    "\n",
    "    time_taken=time.time() - start_time2\n",
    "    print(f\"\\nTime taken for the computation: {'{0:.02f}'.format(time_taken)}\")\n",
    "\n",
    "    # To better handling info_dataset\n",
    "    info_dataset = pd.concat(tmp, sort=False)\n",
    "        \n",
    "    return info_dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
