{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e633794",
   "metadata": {},
   "outputs": [],
   "source": [
    "## libraries for NN driven model\n",
    "# Tensorflow / Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras # for building Neural Networks\n",
    "print('Tensorflow/Keras: %s' % keras.__version__) # print version\n",
    "from keras.models import Sequential # for creating a linear stack of layers for our Neural Network\n",
    "from keras import Input # for instantiating a keras tensor\n",
    "from keras.layers import Dense # for creating regular densely-connected NN layers.\n",
    "import keras.metrics as metrics\n",
    "\n",
    "# Sklearn\n",
    "import sklearn # for model evaluation\n",
    "print('sklearn: %s' % sklearn.__version__) # print version\n",
    "# from sklearn.model_selection import train_test_split # for splitting data into train and test samples\n",
    "# from sklearn.metrics import classification_report # for model evaluation metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b694ce60",
   "metadata": {},
   "source": [
    "## Create and train a neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4c5f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "global loss_fn, optimizer, train_acc_metric\n",
    "\n",
    "# Instantiate an optimizer to train the model.\n",
    "LEARNING_RATE = 0.01\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=LEARNING_RATE)\n",
    "\n",
    "# Instantiate a loss function.\n",
    "loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "# Prepare the metrics.\n",
    "train_acc_metric = tf.keras.metrics.MeanSquaredError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a09e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(network):\n",
    "    \n",
    "    \"\"\"\n",
    "    Create the neural network model to define our TD_ANN_model ode system.\n",
    "    \"\"\"\n",
    "    \n",
    "    model = Sequential()\n",
    "\n",
    "    # Input Layer\n",
    "    model.add(Input(shape=(network[0],), name='Input-Layer'))  #input_shape(1,)\n",
    "\n",
    "#     initializer = keras.initializers.RandomUniform(minval=-1., maxval=1.) # init weights in U[-1,1]\n",
    "    \n",
    "    # Hidden Layers, softplus(x) = log(exp(x) + 1)\n",
    "    for i in range(1,len(network)-1):\n",
    "        model.add(Dense(network[i],\n",
    "                        activation='softplus',\n",
    "                        use_bias=True,\n",
    "                        kernel_initializer = keras.initializers.RandomUniform(minval=-1., maxval=1.),\n",
    "                        bias_initializer = keras.initializers.RandomUniform(minval=-1., maxval=1.),\n",
    "                        trainable = True,\n",
    "                        name=\"Hidden-Layer-\" + str(i)))\n",
    "\n",
    "    # Output Layer, linear(x) = x\n",
    "    model.add(Dense(network[-1],\n",
    "                    activation='linear',\n",
    "                    use_bias=True,\n",
    "                    kernel_initializer= keras.initializers.RandomUniform(minval=-1., maxval=1.),\n",
    "                    bias_initializer = keras.initializers.RandomUniform(minval=-1., maxval=1.),\n",
    "                    trainable=True,\n",
    "                    name='Output-Layer'))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681c561e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_nn(model, X, y, epochs=3, batch_size=10, verbose='auto'):\n",
    "    \n",
    "    \"\"\"\n",
    "    Compile the neural network model and fit using X and y.\n",
    "    \"\"\"\n",
    "\n",
    "    ## Compile keras model\n",
    "    model.compile(optimizer='adam',\n",
    "                 loss=loss_object,      # Loss function to be optimized.\n",
    "                 metrics=[metrics.mae]  # List of metrics to be evaluated by the model during training and testing.\n",
    "                )\n",
    "\n",
    "    ## Fit keras model on the dataset\n",
    "    model.fit(X,           # input data\n",
    "              y,           # target data\n",
    "              batch_size,  # default=32, Number of samples per gradient update.\n",
    "              epochs,      # default=1, Number of epochs to train the model.\n",
    "                           # An epoch is an iteration over the entire x and y data provided\n",
    "              verbose\n",
    "             )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53853c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stats(model,flag_summary=True):\n",
    "    \"\"\"\n",
    "    function to plot stats of our model\n",
    "    \"\"\"\n",
    "    if flag_summary==True:\n",
    "        print(\"\")\n",
    "        print('-------------------- Model Summary --------------------')\n",
    "        model.summary() # print model summary\n",
    "        print(\"\")\n",
    "    print('-------------------- Weights and Biases --------------------')\n",
    "    for layer in model.layers:\n",
    "        print(\"Layer: \", layer.name) # print layer name\n",
    "        print(\"  --Kernels (Weights): \", layer.get_weights()[0]) # kernels (weights)\n",
    "        print(\"  --Biases: \", layer.get_weights()[1]) # biases\n",
    "\n",
    "    print(\"\")\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99cd721c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_train_vars(model):\n",
    "    for var in model.trainable_variables:\n",
    "        print(f\"{var.name}:\\t {var.numpy().flatten()}\")\n",
    "    return\n",
    "\n",
    "def print_grads(model, grads):\n",
    "    print(\"Grads\")\n",
    "    for var, g in zip(model.trainable_variables, grads):\n",
    "        print(f\"{var.name}:\\t {g.numpy().flatten()}\")\n",
    "    return\n",
    "\n",
    "def print_before(model, grads):\n",
    "\n",
    "    print(\"==\"*20)\n",
    "    print(\"Trainable variables (before)\")\n",
    "    print_train_vars(model)\n",
    "    print(\"--\"*20)            \n",
    "\n",
    "    print(\"--\"*40)\n",
    "    print_grads(model, grads)\n",
    "    print(\"--\"*40)\n",
    "\n",
    "    return\n",
    "\n",
    "def print_after(model,grads):\n",
    "    \n",
    "    print(\"--\"*20)\n",
    "    print(\"Trainable variables (after)\")\n",
    "    print_train_vars(model)\n",
    "    print(\"==\"*20)   \n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753a0cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def custom_training(model, X, y, NUM_EPOCHS, step_upd=1):\n",
    "    \n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        #print(f\"Start of epoch {epoch}\")\n",
    "        start_time = time.time()\n",
    "    \n",
    "        # Open a GradientTape to record the operations run\n",
    "        # during the forward pass, which enables auto-differentiation.\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "\n",
    "            # Create tensor that you will watch\n",
    "            x_tensor = tf.convert_to_tensor(X, dtype=tf.float64)\n",
    "            tape.watch(x_tensor)\n",
    "\n",
    "            # Forward pass\n",
    "            y_pred = model(X, training=True)\n",
    "\n",
    "            # Gradient and the corresponding loss function\n",
    "            loss_value = loss_fn(y_true = y, y_pred = y_pred)\n",
    "\n",
    "        # Compute gradients\n",
    "        trainable_vars = model.trainable_variables\n",
    "        grads = tape.gradient(loss_value, trainable_vars)\n",
    "\n",
    "#         # inspect trainable variables and grads\n",
    "#         print_before(model, grads)\n",
    "\n",
    "#         print_grads(model, grads)\n",
    "\n",
    "        # Update weights\n",
    "        if epoch % step_upd == 0:\n",
    "            optimizer.apply_gradients(zip(grads, trainable_vars))\n",
    "\n",
    "#         # inspect trainable variables\n",
    "#         print_after(model,grads)\n",
    "\n",
    "        # Update training metric.\n",
    "        train_acc_metric.update_state(y_true = y, y_pred = y_pred)\n",
    "        \n",
    "    # Display metrics at the end of each epoch.\n",
    "    train_acc = train_acc_metric.result()\n",
    "    print(\"Training acc over epoch: %.4f\" % (float(train_acc),))\n",
    "    print(\"Time taken: %.2fs\" % (time.time() - start_time))\n",
    "\n",
    "    return"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
