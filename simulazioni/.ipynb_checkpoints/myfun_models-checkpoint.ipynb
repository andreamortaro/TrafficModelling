{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d554e98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-29 12:51:14.017963: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow/Keras: 2.11.0\n",
      "sklearn: 1.3.0\n"
     ]
    }
   ],
   "source": [
    "## ODE\n",
    "from scipy.integrate import odeint, solve_ivp\n",
    "import numpy as np\n",
    "import pandas as pd # for data manipulation\n",
    "import time\n",
    "\n",
    "from ipynb.fs.full.myfun_nn import *\n",
    "from ipynb.fs.defs.myfun_plot import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27b63b7",
   "metadata": {},
   "source": [
    "# Define the LWR models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ddce756",
   "metadata": {},
   "source": [
    "## Lin and Log models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d6718d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Traffic dynamic with LWR-model\n",
    "\n",
    "def TD_LWR_model(t, X, N, v0, L, flag):\n",
    "    \"\"\"\n",
    "    Lighthill-Whitham-Richards (LWR) traffic flow model in 1D\n",
    "    \"\"\"\n",
    "    #N, v0, L, flag = args[0], args[1], args[2], args[3]\n",
    "    \n",
    "    # W function\n",
    "    match flag:\n",
    "        case \"Lin\":\n",
    "            W = lambda z: v0*(1-1/z)\n",
    "        case \"Log\":\n",
    "            W = lambda z: v0*np.log(z)\n",
    "        case _:\n",
    "            return f\"No match for {flag}, you can only choose between \\\"Lin\\\" and \\\"Log\\\"\"\n",
    "\n",
    "    # ode sys\n",
    "    d_x = np.zeros(N)\n",
    "    \n",
    "    for i in range(0,N-1):\n",
    "        tmp = (X[i+1] - X[i])/L\n",
    "        d_x[i] = W(tmp)\n",
    "\n",
    "    d_x[N-1] = v0\n",
    "        \n",
    "    return d_x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d6b935",
   "metadata": {},
   "source": [
    "## NN driven model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1bd98145",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Traffic dynamic with ANN\n",
    "\n",
    "def TD_ANN_model(t, X, vel):\n",
    "    \n",
    "    \"\"\"\n",
    "    Lighthill-Whitham-Richards (LWR) traffic flow model in 1D\n",
    "    Transform a list as vel into a function of t,X.\n",
    "    \"\"\"\n",
    "    \n",
    "    d_x = vel\n",
    "        \n",
    "    return d_x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44fc198",
   "metadata": {},
   "source": [
    "### Ode solver for the NN driven model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62becfdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_discretization(t0, tend, deltat=0.05):\n",
    "    \n",
    "    Nt = round((tend-t0)/deltat) + 1               # number of discretization points\n",
    "                                                   # cast the value into int, us round to avoid cast problem\n",
    "    tspan = np.linspace(t0, tend, int(Nt))         # timespan\n",
    "    \n",
    "    return tspan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c299620",
   "metadata": {},
   "outputs": [],
   "source": [
    "def odesolver_ann(x0, vel, t0, tend, deltat = 0.05):\n",
    "    \n",
    "    \"\"\"\n",
    "    odesolver_ann solves the TD_ANN_model ode system:\n",
    "    * in [t0, tend] with timestep as deltat,\n",
    "    * starting from x0\n",
    "    * vel is the rhs passed to TD_ANN_model to create the model\n",
    "    \"\"\"\n",
    "    \n",
    "    tspan_ann = time_discretization(t0,tend,deltat)\n",
    "        \n",
    "    sol_ann = odeint(TD_ANN_model, x0, tspan_ann, args=(vel,), tfirst = True).T\n",
    "\n",
    "    return tspan_ann, sol_ann"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00022eda",
   "metadata": {},
   "source": [
    "### Useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56a45aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_data_ann_scene_std(scn):\n",
    "    \n",
    "#     \"\"\"\n",
    "#     create_data_ann_scene gives the X and y for an entire scene\n",
    "    \n",
    "#     X_scn, y_scn = create_data_ann_scene(scn)\n",
    "    \n",
    "#     X_scn is a list of consecutive distances btw the vehicles of a scene, at each timestamps\n",
    "#     y_scn is a list of approximated velocities, of all the vehicles except the leader one.\n",
    "#     \"\"\"\n",
    "    \n",
    "#     ## Create X\n",
    "#     X_scn = scn['cons_dis_std']\n",
    "    \n",
    "#     ## Create y\n",
    "#     dX_scn = np.diff(scn['Xarr_std'],axis=1)\n",
    "#     dT_scn = np.diff(scn['Tarr'])\n",
    "#     velocity = dX_scn/dT_scn # velocity at the timestamps\n",
    "\n",
    "#     # we choose the first velocity discretized as (x_(i+1)-x_i)/deltaT\n",
    "#     y_scn = velocity[:-1] #drop the last vehicle\n",
    "    \n",
    "#     return X_scn, y_scn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3dcde325",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_ann_scene(scn):\n",
    "    \n",
    "    \"\"\"\n",
    "    create_data_ann_scene gives the X and y for an entire scene\n",
    "    \n",
    "    X_scn, y_scn = create_data_ann_scene(scn)\n",
    "    \n",
    "    X_scn is a list of consecutive distances btw the vehicles of a scene, at each timestamps\n",
    "    y_scn is a list of approximated velocities, of all the vehicles except the leader one.\n",
    "    \"\"\"\n",
    "\n",
    "    ## Create X\n",
    "    X_scn = scn['cons_dis']\n",
    "\n",
    "    ## Create y\n",
    "    dX_scn = np.diff(scn['Xarr'],axis=1)\n",
    "    dT_scn = np.diff(scn['Tarr'])\n",
    "    velocity = dX_scn/dT_scn # velocity at the timestamps\n",
    "\n",
    "    # we choose the first velocity discretized as (x_(i+1)-x_i)/deltaT\n",
    "    y_scn = velocity[:-1] #drop the last vehicle\n",
    "    \n",
    "    return X_scn, y_scn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff1323fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq2scn(df):\n",
    "    \n",
    "    \"\"\"\n",
    "    get an array of scenes, pandas obj\n",
    "    \"\"\"\n",
    "    \n",
    "    seq = []\n",
    "    \n",
    "    # extract input and target for each scene\n",
    "    for row in df.iterrows(): #run over rows\n",
    "        scn = row[1]\n",
    "        seq.append(scn)\n",
    "\n",
    "    return seq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04749ec5",
   "metadata": {},
   "source": [
    "### Solve the NN-driven model in a scene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "80cbf531",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_timestamps_scene(t, x, deltat = 0.05):\n",
    "    \n",
    "    \"\"\"\n",
    "    Match the computed solution to the same timestamps of the scene\n",
    "    \n",
    "    t_matched, x_matched = match_timestamps_scene(t, x, deltat = 0.05)\n",
    "    \"\"\"\n",
    "    # To recover the same timestep in the data\n",
    "    factor = int(0.2/deltat)\n",
    "    \n",
    "    t_matched = np.array(t)[::factor]\n",
    "    x_matched = np.array([traj[::factor] for traj in x])\n",
    "    \n",
    "    return t_matched, x_matched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0ec84188",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_sol_lists(N, tspan_ann, sol_ann, x_list, t_list):\n",
    "    \n",
    "    \"\"\"\n",
    "    Once you solve the ode in a sub-interval of a scene, you update the lists containing t,x\n",
    "    \n",
    "    x_list, t_list = update_sol_lists(N, tspan_ann, sol_ann, x_list, t_list)\n",
    "    \"\"\"\n",
    "    \n",
    "    x_ann = sol_ann.tolist()\n",
    "    t_ann = tspan_ann[1:] # avoid the first recording\n",
    "\n",
    "    # add sol to the correct veh\n",
    "    for j in range(0,N):\n",
    "        tmp = x_ann[j][1:] # avoid the first recording\n",
    "        x_list[j] = np.concatenate([x_list[j],tmp])\n",
    "    t_list = np.concatenate([t_list,t_ann]).tolist()\n",
    "    \n",
    "    return x_list, t_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eac22c3",
   "metadata": {},
   "source": [
    "### Default training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f55333fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_nn_scn_default(nn_model, scn, epochs, batch_size, v0, deltat=0.05, verbose=\"auto\"):\n",
    "    \n",
    "    \"\"\"\n",
    "    t_ann_list, x_ann_list, vel_ann_list = odesolver_ann_scene(nn_model, scn, epochs, batch_size, v0, deltat=0.05, verbose=\"auto\")\n",
    "    \n",
    "    odesolver_ann_scene solve the ode model driven by a nn in a scene.\n",
    "    \"\"\"\n",
    " \n",
    "    x_list, t_list, v_list = [[i] for i in scn['Xarr'][:,0]], [scn['Tarr'][0]], []\n",
    "    N, tstamps, fmt = scn['N. vehicles'], scn['Tarr'], '{0:.02f}'\n",
    "\n",
    "    X_arr, y_arr = create_data_ann_scene(scn)\n",
    "\n",
    "    print(\"==\"*50)\n",
    "    print(f\"We have {len(tstamps)-1} time intervals inside [{fmt.format(tstamps[0])},{fmt.format(tstamps[-1])}]\\n\")\n",
    "    \n",
    "    for i in range(0,len(tstamps)-1):\n",
    "\n",
    "        print(\"--\"*50)\n",
    "        print(f\"Time interval n.{i}: [{fmt.format(scn['Tarr'][i])}, {fmt.format(scn['Tarr'][i+1])}]\\n\")\n",
    "        \n",
    "        ## STEP 1: Train the NN model and predict the rhs of the TD_ANN_model\n",
    "        X, y = X_arr[:,i], y_arr[:,i]\n",
    "\n",
    "        train_nn(nn_model, X, y, epochs, batch_size, verbose)\n",
    "        y_pred = nn_model(X, training=True).numpy().flatten().tolist()\n",
    "\n",
    "        ## STEP 2: Solve the ODE sys in this time interval\n",
    "        x0 = [l[-1] for l in np.vstack(x_list).tolist()]  # last values computed\n",
    "        t0, tend = scn['Tarr'][i], scn['Tarr'][i+1]\n",
    "        v_ann = np.append(y_pred,v0).tolist()\n",
    "        tspan_ann, sol_ann = odesolver_ann(x0, v_ann, t0, tend, deltat=0.05)\n",
    "\n",
    "        print(f\"\\\n",
    "        * X: {X}\\n\\\n",
    "        * y_true: {y}\\n\\\n",
    "        * y_pred: {y_pred}\\n\\\n",
    "        * v_ann: {v_ann}\\n\")\n",
    "        \n",
    "        ## STEP 3: store info\n",
    "        x_list, t_list = update_sol_lists(N, tspan_ann, sol_ann, x_list, t_list)\n",
    "        v_list.append(v_ann)\n",
    "\n",
    "        print(\"--\"*50)\n",
    "    \n",
    "    print(\"==\"*50)\n",
    "    \n",
    "    return t_list, x_list, v_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36030428",
   "metadata": {},
   "source": [
    "### Custom training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1c3447d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_nn_scn_custom(model, scn, v0, LEARNING_RATE_NN):\n",
    "    \n",
    "    \"\"\"\n",
    "    t_list, x_list, v_list = solve_nn_scn_custom(model, scn, v0, LEARNING_RATE_NN)\n",
    "    \"\"\"\n",
    "    \n",
    "    x_list, t_list, v_list = [[i] for i in scn['Xarr'][:,0]], [scn['Tarr'][0]], []\n",
    "    N, tstamps, fmt = scn['N. vehicles'], scn['Tarr'], '{0:.02f}'\n",
    "    \n",
    "    X_arr, y_arr = create_data_ann_scene(scn)\n",
    "    \n",
    "    print(\"==\"*50)\n",
    "    print(f\"We have {len(tstamps)-1} time intervals inside [{fmt.format(tstamps[0])},{fmt.format(tstamps[-1])}]\\n\")\n",
    "    \n",
    "    # for nn training\n",
    "    optimizer = tf.keras.optimizers.SGD(learning_rate=LEARNING_RATE_NN)\n",
    "    loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "    \n",
    "    for i in range(0,len(tstamps)-1):\n",
    "\n",
    "        print(\"--\"*50)\n",
    "        print(f\"Time interval n.{i}: [{fmt.format(scn['Tarr'][i])}, {fmt.format(scn['Tarr'][i+1])}]\\n\")\n",
    "        \n",
    "        ## STEP 1: Create the dataset and train the nn model\n",
    "        X, y = X_arr[:,i], y_arr[:,i]\n",
    "\n",
    "        # Solve the ODE sys in this time interval\n",
    "        x0 = [l[-1] for l in np.vstack(x_list).tolist()]  # last values computed\n",
    "        t0, tend = scn['Tarr'][i], scn['Tarr'][i+1]\n",
    "#         vel = np.append(y,v0)\n",
    "        \n",
    "#         print(f\"\\\n",
    "#         * x0: {x0}\\n\\\n",
    "#         * vel: {vel}\\n\")\n",
    "#         tspan_ann, sol_ann = odesolver_ann(x0, vel, t0, tend, deltat=0.05)\n",
    "        \n",
    "#         true_trajs = scn['Xarr'][:-1]\n",
    "\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "\n",
    "            # Create tensor that you will watch\n",
    "            x_tensor = tf.convert_to_tensor(X, dtype=tf.float64)\n",
    "            tape.watch(x_tensor)\n",
    "\n",
    "            y_pred = model(X, training=True) # forward pass\n",
    "            loss_value = loss_fn(y_true = y, y_pred = y_pred) # loss function\n",
    "\n",
    "        # Compute gradients\n",
    "        trainable_vars = model.trainable_variables\n",
    "        grads = tape.gradient(loss_value, trainable_vars)\n",
    "\n",
    "        # Update weights\n",
    "        optimizer.apply_gradients(zip(grads, trainable_vars))\n",
    "\n",
    "#             print_grads(model, grads) # plot the last grads wrt trainable vars\n",
    "\n",
    "        ## Update the solution, changing velocities\n",
    "        v_ann = np.append(y_pred.numpy().flatten().tolist(),v0).tolist()\n",
    "        tspan_ann, sol_ann = odesolver_ann(x0, v_ann, t0, tend, deltat=0.05)\n",
    "\n",
    "        print(f\"\\\n",
    "        * X: {X}\\n\\\n",
    "        * y_true: {y}\\n\\\n",
    "        * y_pred: {y_pred.numpy().flatten().tolist()}\\n\\\n",
    "        * v_ann: {v_ann}\\n\")\n",
    "        \n",
    "        ## STEP 2: Store the info\n",
    "        x_list, t_list = update_sol_lists(N, tspan_ann, sol_ann, x_list, t_list)\n",
    "        v_list.append(v_ann)\n",
    "\n",
    "\n",
    "        print(\"--\"*50)\n",
    "    \n",
    "    print(\"==\"*50)\n",
    "    \n",
    "    return t_list, x_list, v_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3e52f5",
   "metadata": {},
   "source": [
    "### Solve the nn-driven model in the all the scenes in a df, and get v0 mean for each scene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f8298048",
   "metadata": {},
   "outputs": [],
   "source": [
    "global loss_objective\n",
    "\n",
    "loss_objective = tf.keras.losses.MeanSquaredError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "538d271f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SGD_v0(scn, x_list_matched, v0, LEARNING_RATE_v0):\n",
    "    \n",
    "    \"\"\"\n",
    "    v0_upd, loss_val, grads, g = SGD_v0(scn, x_list_matched, v0, LEARNING_RATE_v0)\n",
    "    \"\"\"\n",
    "    \n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "\n",
    "        trajs_true_tensor = tf.convert_to_tensor(scn['Xarr'], dtype=tf.float64)\n",
    "\n",
    "        # Create tensor that you will watch\n",
    "        trajs_pred_tensor = tf.convert_to_tensor(x_list_matched, dtype=tf.float64)\n",
    "        tape.watch(trajs_pred_tensor)\n",
    "\n",
    "        loss_val = loss_objective(y_true = trajs_true_tensor, y_pred = trajs_pred_tensor)\n",
    "\n",
    "    # Compute gradients\n",
    "    grads = tape.gradient(loss_val, trajs_pred_tensor)\n",
    "\n",
    "    # Update weights\n",
    "    g = grads[-1].numpy()[1:].mean() # I'm watching at a mean velocity of the leader car..\n",
    "    v0_upd = v0 - LEARNING_RATE_v0*g\n",
    "    \n",
    "    return v0_upd, loss_val, grads, g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5d32dbce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_nn_df(df, model, v0_guess=30, NUM_EPOCHS=10, LEARNING_RATE_NN=0.01, LEARNING_RATE_v0=0.5, flag_print=False):\n",
    "    \n",
    "    \"\"\"\n",
    "    Solve nn in a single df ang get info_df, which gives info about v0_scn_mean for each scene.\n",
    "    \n",
    "    info_df = solve_nn_df(df, model, v0_guess=30,\n",
    "                          NUM_EPOCHS=10, LEARNING_RATE_NN=0.01, LEARNING_RATE_v0=0.5, flag_print=False)\n",
    "    \"\"\"\n",
    "    \n",
    "    scn_list = seq2scn(df)\n",
    "    info_df, fmt = [], '{0:.02f}'\n",
    "    \n",
    "    for scn in scn_list:\n",
    "\n",
    "        v0, v0_scn = v0_guess, []\n",
    "        tstamps = scn['Tarr']\n",
    "\n",
    "        print(f\"\\nScene n.{scn.name}/{len(scn_list)}, time interval:\\\n",
    "[{fmt.format(tstamps[0])},{fmt.format(tstamps[-1])}]\")\n",
    "\n",
    "        for epoch in range(NUM_EPOCHS+1):\n",
    "\n",
    "            t_list, x_list, _ = solve_nn_scn_custom(model, scn, v0, LEARNING_RATE_NN)\n",
    "            _, x_list_matched = match_timestamps_scene(t_list, x_list)\n",
    "\n",
    "            # Append v0 used, before to update it\n",
    "            v0_scn.append(v0)\n",
    "\n",
    "            # Update v0 with SGD\n",
    "            v0_upd, loss_val, grads, g = SGD_v0(scn, x_list_matched, v0, LEARNING_RATE_v0)\n",
    "            v0 = v0_upd\n",
    "\n",
    "#             if epoch % nprint == 0:\n",
    "#                 print(\"--\"*50)\n",
    "#                 print(f\"Epoch n.{epoch}\")\n",
    "\n",
    "#                 # plot function\n",
    "#                 t_ann_matched, trajs_ann_matched = match_timestamps_scene(t_list, x_list)\n",
    "#                 tscale = 1+(tstamps[-1]-tstamps[0])/20000\n",
    "#                 title = f\"Scene n. {scn.name}, at epoch n.{epoch}\"\n",
    "#                 plot_scn(scn, trajs_ann_matched, title, xbal=0.01, ybal=0.05, scale=tscale)\n",
    "\n",
    "#                 print(f\"\\n\\\n",
    "#                 Loss: {loss_val}\\n\\\n",
    "#                 gradient to update v0: {g}\\n\\\n",
    "#                 v0 updated: {v0}\")\n",
    "#                 print(\"--\"*50)\n",
    "\n",
    "        v0_scn_mean = np.array(v0_scn).mean()\n",
    "        info_df.append([scn.name,v0_scn_mean])\n",
    "\n",
    "        # plot function\n",
    "        if flag_print:\n",
    "            \n",
    "            print(\"--\"*50)\n",
    "            \n",
    "            t_list_matched, x_list_matched = match_timestamps_scene(t_list, x_list)\n",
    "            tscale = 1+(tstamps[-1]-tstamps[0])/20000\n",
    "            title = f\"$Scene\\ n.{scn.name}/{len(scn_list)},\\ using\\ {epoch}\\ epochs$\"\n",
    "            plot_scn(scn, x_list_matched, title, xbal=0.01, ybal=0.05, scale=tscale)\n",
    "            \n",
    "            print(f\"\\\n",
    "            Loss: {loss_val}\\n\\\n",
    "            v0 updated: {v0}\")\n",
    "            \n",
    "            print(\"--\"*50)\n",
    "\n",
    "    return info_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1085ae",
   "metadata": {},
   "source": [
    "### Solve NN driven model in each df of a dataset, and get v0 mean for each scn in each df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ed2b6c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def v0_dataset(model, v0_guess, dataset, NUM_EPOCHS, LEARNING_RATE_NN, LEARNING_RATE_v0, flag_print):\n",
    "    \n",
    "    \"\"\"\n",
    "    v0_dflist = v0_dataset(model, v0_guess, dataset, NUM_EPOCHS, LEARNING_RATE_NN, LEARNING_RATE_v0, flag_print)\n",
    "    \n",
    "    For each df in dataset compute v0 for each scene in df. Return v0_dflist.\n",
    "    \"\"\"\n",
    "    \n",
    "    v0_dflist = []\n",
    "    \n",
    "    for step, df in enumerate(dataset):\n",
    "\n",
    "        v0 = v0_guess # in every scn I start to approximate with v0 guess\n",
    "        \n",
    "        start_time1 = time.time()\n",
    "\n",
    "        print(\"==\"*30)\n",
    "        \n",
    "        print(f\"{step}/{len(dataset)}\\n\\\n",
    "        In df n.{df['N. file'][0]} we have {len(df)} scenes\")\n",
    "        print(\"--\"*30)\n",
    "        info_df = solve_nn_df(df, model, v0, NUM_EPOCHS, LEARNING_RATE_NN, LEARNING_RATE_v0, flag_print)\n",
    "\n",
    "        v0_list = [l[1] for l in info_df]\n",
    "        v0_mean = np.array(v0_list).mean()\n",
    "\n",
    "        v0_df = pd.DataFrame({'v0_list': v0_list,\\\n",
    "                              'v0_mean_df': v0_mean,\\\n",
    "                              'n_scn': len(df),\\\n",
    "                              'N. file': df['N. file'][0]})\n",
    "\n",
    "        v0_dflist.append(v0_df)\n",
    "\n",
    "        print(f\"\\nv0_mean in this df is: {v0_mean}\")\n",
    "        print(\"Time taken: %.2fs\" % (time.time() - start_time1))\n",
    "\n",
    "        print_train_vars(model)\n",
    "\n",
    "        print(\"--\"*30)\n",
    "        \n",
    "    return v0_dflist"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
