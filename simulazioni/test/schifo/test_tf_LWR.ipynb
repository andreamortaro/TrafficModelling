{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9102c8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-26 11:56:49.612676: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(123)\n",
    "tf.random.set_seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ce88f4",
   "metadata": {},
   "source": [
    "## Defining Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22e0dc68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial condition\n",
    "f0 = 1\n",
    "# infinitesimal small number\n",
    "inf_s = np.sqrt(np.finfo(np.float32).eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4d09e2",
   "metadata": {},
   "source": [
    "$$\\frac{dg(x)}{x} = lim_{n\\to-\\infty} \\frac{g(x+n)-g(x)}{n},$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c0ab078",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.01\n",
    "training_steps = 500\n",
    "batch_size = 100\n",
    "display_step = training_steps/10\n",
    "\n",
    "# Network Parameters\n",
    "n_input = 1     # input layer number of neurons\n",
    "n_hidden_1 = 32 # 1st layer number of neurons\n",
    "n_hidden_2 = 32 # 2nd layer number of neurons\n",
    "n_output = 1    # output layer number of neurons\n",
    "\n",
    "weights = {\n",
    "    'h1': tf.Variable(tf.random.normal([n_input, n_hidden_1])),\n",
    "    'h2': tf.Variable(tf.random.normal([n_hidden_1, n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random.normal([n_hidden_2, n_output]))\n",
    "}\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random.normal([n_hidden_1])),\n",
    "    'b2': tf.Variable(tf.random.normal([n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random.normal([n_output]))\n",
    "}\n",
    "\n",
    "# Stochastic gradient descent optimizer.\n",
    "optimizer = tf.optimizers.SGD(learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893f9fc6",
   "metadata": {},
   "source": [
    "## Defining the Model and Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329e0975",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fbd1f286",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "def multilayer_perceptron(x):\n",
    "    x = np.array([[[x]]],  dtype='float32')\n",
    "    # Hidden fully connected layer with 32 neurons\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "    layer_1 = tf.nn.sigmoid(layer_1)\n",
    "    # Hidden fully connected layer with 32 neurons\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
    "    layer_2 = tf.nn.sigmoid(layer_2)\n",
    "    # Output fully connected layer\n",
    "    output = tf.matmul(layer_2, weights['out']) + biases['out']\n",
    "    return output\n",
    "\n",
    "# Universal Approximator\n",
    "def g(x):\n",
    "    return x * multilayer_perceptron(x) + f0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9de334",
   "metadata": {},
   "source": [
    "### ODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8508c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 5 # number of eqs\n",
    "L = 5 # initial length..\n",
    "v0 = 3 # velocity of first car\n",
    "\n",
    "# LWR model\n",
    "def f(X):    \n",
    "    eqs = np.zeros(N)\n",
    "    \n",
    "    for i in range(0,N-1):\n",
    "        eqs[i] = W*X[i+1] - X[i]\n",
    "    eqs[N-1] = v0\n",
    "        \n",
    "    return eqs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3f5c7b",
   "metadata": {},
   "source": [
    "### Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c80cb2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom loss function to approximate the derivatives\n",
    "def custom_loss():\n",
    "    summation = []\n",
    "    for x in np.linspace(0,1,10):\n",
    "        dNN = (g(x+inf_s)-g(x))/inf_s\n",
    "        summation.append((dNN - f(x))**2)\n",
    "    return tf.reduce_sum(tf.abs(summation))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95d7c38",
   "metadata": {},
   "source": [
    "## Train Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "73bbcd44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step():\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = custom_loss()\n",
    "    trainable_variables = list(weights.values()) + list(biases.values())\n",
    "    gradients = tape.gradient(loss, trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "425f98b4",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "invalid index to scalar variable.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(training_steps):\n\u001b[0;32m----> 2\u001b[0m     \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m%\u001b[39m display_step \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m      4\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss: \u001b[39m\u001b[38;5;132;01m%f\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (custom_loss()))\n",
      "Cell \u001b[0;32mIn[7], line 3\u001b[0m, in \u001b[0;36mtrain_step\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_step\u001b[39m():\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mGradientTape() \u001b[38;5;28;01mas\u001b[39;00m tape:\n\u001b[0;32m----> 3\u001b[0m         loss \u001b[38;5;241m=\u001b[39m \u001b[43mcustom_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     trainable_variables \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(weights\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlist\u001b[39m(biases\u001b[38;5;241m.\u001b[39mvalues())\n\u001b[1;32m      5\u001b[0m     gradients \u001b[38;5;241m=\u001b[39m tape\u001b[38;5;241m.\u001b[39mgradient(loss, trainable_variables)\n",
      "Cell \u001b[0;32mIn[6], line 6\u001b[0m, in \u001b[0;36mcustom_loss\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m np\u001b[38;5;241m.\u001b[39mlinspace(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m10\u001b[39m):\n\u001b[1;32m      5\u001b[0m     dNN \u001b[38;5;241m=\u001b[39m (g(x\u001b[38;5;241m+\u001b[39minf_s)\u001b[38;5;241m-\u001b[39mg(x))\u001b[38;5;241m/\u001b[39minf_s\n\u001b[0;32m----> 6\u001b[0m     summation\u001b[38;5;241m.\u001b[39mappend((dNN \u001b[38;5;241m-\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mreduce_sum(tf\u001b[38;5;241m.\u001b[39mabs(summation))\n",
      "Cell \u001b[0;32mIn[5], line 11\u001b[0m, in \u001b[0;36mf\u001b[0;34m(X)\u001b[0m\n\u001b[1;32m      8\u001b[0m d_x \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(N)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m,N\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m---> 11\u001b[0m     tmp \u001b[38;5;241m=\u001b[39m (\u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m-\u001b[39m X[i])\u001b[38;5;241m/\u001b[39mL\n\u001b[1;32m     12\u001b[0m     d_x[i] \u001b[38;5;241m=\u001b[39m W(tmp)\n\u001b[1;32m     14\u001b[0m d_x[N\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m v0\n",
      "\u001b[0;31mIndexError\u001b[0m: invalid index to scalar variable."
     ]
    }
   ],
   "source": [
    "for i in range(training_steps):\n",
    "    train_step()\n",
    "    if i % display_step == 0:\n",
    "        print(\"loss: %f \" % (custom_loss()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1542f5e3",
   "metadata": {},
   "source": [
    "## Plot the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27bc97fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import figure\n",
    "\n",
    "figure(figsize=(10,10))\n",
    "# True Solution (found analitically)\n",
    "def true_solution(x):\n",
    "    return x**2 + 1\n",
    "\n",
    "X = np.linspace(0, 1, 100)\n",
    "result = []\n",
    "for i in X:\n",
    "  # result.append(f(i))\n",
    "  result.append(g(i).numpy()[0][0][0])\n",
    "\n",
    "S = true_solution(X)\n",
    "  \n",
    "plt.plot(X, S, label=\"Original Function\")\n",
    "plt.plot(X, result, label=\"Neural Net Approximation\")\n",
    "plt.legend(loc=2, prop={'size': 20})\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
